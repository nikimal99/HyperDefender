{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0Jjhp6WSPvi"
      },
      "source": [
        "## Requirements ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2q2PIF4SFvh",
        "outputId": "06e6520d-1c7c-4847-c780-0ac1b1f3a09b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch==1.11.0\n",
            "  Downloading torch-1.11.0-cp310-cp310-manylinux1_x86_64.whl (750.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.6/750.6 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.11.0) (4.11.0)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.2.1+cu121\n",
            "    Uninstalling torch-2.2.1+cu121:\n",
            "      Successfully uninstalled torch-2.2.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 1.11.0 which is incompatible.\n",
            "torchdata 0.7.1 requires torch>=2, but you have torch 1.11.0 which is incompatible.\n",
            "torchtext 0.17.1 requires torch==2.2.1, but you have torch 1.11.0 which is incompatible.\n",
            "torchvision 0.17.1+cu121 requires torch==2.2.1, but you have torch 1.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.11.0\n",
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m831.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore) (1.25.2)\n",
            "Collecting yacs>=0.1.6 (from fvcore)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (6.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore) (4.66.4)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (2.4.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore) (9.4.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.9.0)\n",
            "Collecting iopath>=0.1.7 (from fvcore)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath>=0.1.7->fvcore) (4.11.0)\n",
            "Collecting portalocker (from iopath>=0.1.7->fvcore)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=33be8760c14f03b6d6b2bf643bcc666e842b09376593f86e9d7ce2d6ded81cdf\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=c8c85e74c76ceea975c5a9740a6a567460014ec7b6f2ebcd3f48eaec2bd7d909\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, iopath, fvcore\n",
            "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-2.8.2 yacs-0.1.8\n",
            "Collecting tensorly\n",
            "  Downloading tensorly-0.8.1-py3-none-any.whl (229 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.7/229.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorly) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from tensorly) (1.11.4)\n",
            "Installing collected packages: tensorly\n",
            "Successfully installed tensorly-0.8.1\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "!pip install torch==1.11.0\n",
        "!pip install -U fvcore\n",
        "\n",
        "!pip install tensorly\n",
        "# !pip install https://data.pyg.org/whl/torch-1.11.0%2Bcu102/pyg_lib-0.1.0%2Bpt111cu102-cp310-cp310-linux_x86_64.whl\n",
        "# !pip install https://data.pyg.org/whl/torch-1.11.0%2Bcu102/torch_scatter-2.0.9-cp310-cp310-linux_x86_64.whl\n",
        "# !pip install torchviz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2HCjo7yaQgf"
      },
      "source": [
        "\n",
        "###Manifolds###\n",
        "\n",
        "Poincare, Lorentz and Euclidean Manifolds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0k-0tblaUVL"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "#!/usr/bin/env python3\n",
        "# Copyright (c) 2018-present, Facebook, Inc.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "\n",
        "##########Manifold#######################\n",
        "\n",
        "# @title\n",
        "#!/usr/bin/env python3\n",
        "# Copyright (c) 2018-present, Facebook, Inc.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "\n",
        "##########Manifold#######################\n",
        "\n",
        "\n",
        "\n",
        "from abc import abstractmethod\n",
        "from torch.nn import Embedding\n",
        "\n",
        "\n",
        "class Manifold(object):\n",
        "    def allocate_lt(self, N, dim, sparse):\n",
        "        return Embedding(N, dim, sparse=sparse)\n",
        "\n",
        "    def normalize(self, u):\n",
        "        return u\n",
        "\n",
        "    @abstractmethod\n",
        "    def distance(self, u, v):\n",
        "        \"\"\"\n",
        "        Distance function\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def init_weights(self, w, scale=1e-4):\n",
        "        w.weight.data.uniform_(-scale, scale)\n",
        "\n",
        "    @abstractmethod\n",
        "    def expm(self, p, d_p, lr=None, out=None):\n",
        "        \"\"\"\n",
        "        Exponential map\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def logm(self, x, y):\n",
        "        \"\"\"\n",
        "        Logarithmic map\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def ptransp(self, x, y, v, ix=None, out=None):\n",
        "        \"\"\"\n",
        "        Parallel transport\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def norm(self, u, **kwargs):\n",
        "        if isinstance(u, Embedding):\n",
        "            u = u.weight\n",
        "        return u.pow(2).sum(dim=-1).sqrt()\n",
        "\n",
        "    @abstractmethod\n",
        "    def half_aperture(self, u):\n",
        "        \"\"\"\n",
        "        Compute the half aperture of an entailment cone.\n",
        "        As in: https://arxiv.org/pdf/1804.01882.pdf\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def angle_at_u(self, u, v):\n",
        "        \"\"\"\n",
        "        Compute the angle between the two half lines (0u and uv\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "#############Euclidean Manifold####################\n",
        "#!/usr/bin/env python3\n",
        "# Copyright (c) 2018-present, Facebook, Inc.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "\n",
        "import torch as th\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class EuclideanManifold(Manifold):\n",
        "    __slots__ = [\"max_norm\"]\n",
        "\n",
        "    def __init__(self, max_norm=None, K=None, **kwargs):\n",
        "        self.max_norm = max_norm\n",
        "        self.K = K\n",
        "        if K is not None:\n",
        "            self.inner_radius = 2 * self.K / (1 + np.sqrt(1 + 4 * self.K * self.K))\n",
        "\n",
        "    def normalize(self, u):\n",
        "        d = u.size(-1)\n",
        "        if self.max_norm:\n",
        "            u.view(-1, d).renorm_(2, 0, self.max_norm)\n",
        "        return u\n",
        "\n",
        "    def distance(self, u, v):\n",
        "        return (u - v).pow(2).sum(dim=-1)\n",
        "\n",
        "    def rgrad(self, p, d_p):\n",
        "        return d_p\n",
        "\n",
        "    def half_aperture(self, u):\n",
        "        sqnu = u.pow(2).sum(dim=-1)\n",
        "        return th.asin(self.inner_radius / sqnu.sqrt())\n",
        "\n",
        "    def angle_at_u(self, u, v):\n",
        "        norm_u = self.norm(u)\n",
        "        norm_v = self.norm(v)\n",
        "        dist = self.distance(v, u)\n",
        "        num = norm_u.pow(2) - norm_v.pow(2) - dist.pow(2)\n",
        "        denom = 2 * norm_v * dist\n",
        "        return (num / denom).acos()\n",
        "\n",
        "    def expm(self, p, d_p, normalize=False, lr=None, out=None):\n",
        "        if lr is not None:\n",
        "            d_p.mul_(-lr)\n",
        "        if out is None:\n",
        "            out = p\n",
        "        out.add_(d_p)\n",
        "        if normalize:\n",
        "            self.normalize(out)\n",
        "        return out\n",
        "\n",
        "    def logm(self, p, d_p, out=None):\n",
        "        return p - d_p\n",
        "\n",
        "    def ptransp(self, p, x, y, v):\n",
        "        ix, v_ = v._indices().squeeze(), v._values()\n",
        "        return p.index_copy_(0, ix, v_)\n",
        "\n",
        "########################Poincare Manifold############################\n",
        "#!/usr/bin/env python3\n",
        "# Copyright (c) 2018-present, Facebook, Inc.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "\n",
        "import torch as th\n",
        "from torch.autograd import Function\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class PoincareManifold(EuclideanManifold):\n",
        "    def __init__(self, eps=1e-5, K=None, **kwargs):\n",
        "        self.eps = eps\n",
        "        super(PoincareManifold, self).__init__(max_norm=1 - eps)\n",
        "        self.K = K\n",
        "        if K is not None:\n",
        "            self.inner_radius = 2 * K / (1 + np.sqrt(1 + 4 * K * self.K))\n",
        "\n",
        "    def distance(self, u, v):\n",
        "        return Distance.apply(u, v, self.eps)\n",
        "\n",
        "    def half_aperture(self, u):\n",
        "        eps = self.eps\n",
        "        sqnu = u.pow(2).sum(dim=-1)\n",
        "        sqnu.clamp_(min=0, max=1 - eps)\n",
        "        return th.asin((self.inner_radius * (1 - sqnu) / th.sqrt(sqnu))\n",
        "            .clamp(min=-1 + eps, max=1 - eps))\n",
        "\n",
        "    def angle_at_u(self, u, v):\n",
        "        norm_u = u.norm(2, dim=-1)\n",
        "        norm_v = v.norm(2, dim=-1)\n",
        "        dot_prod = (u * v).sum(dim=-1)\n",
        "        edist = (u - v).norm(2, dim=-1)  # euclidean distance\n",
        "        num = (dot_prod * (1 + norm_v ** 2) - norm_v ** 2 * (1 + norm_u ** 2))\n",
        "        denom = (norm_v * edist * (1 + norm_v**2 * norm_u**2 - 2 * dot_prod).sqrt())\n",
        "        return (num / denom).clamp_(min=-1 + self.eps, max=1 - self.eps).acos()\n",
        "\n",
        "    def rgrad(self, p, d_p):\n",
        "        if d_p.is_sparse:\n",
        "            p_sqnorm = th.sum(\n",
        "                p[d_p._indices()[0].squeeze()] ** 2, dim=1,\n",
        "                keepdim=True\n",
        "            ).expand_as(d_p._values())\n",
        "            n_vals = d_p._values() * ((1 - p_sqnorm) ** 2) / 4\n",
        "            n_vals.renorm_(2, 0, 5)\n",
        "            d_p = th.sparse.DoubleTensor(d_p._indices(), n_vals, d_p.size())\n",
        "        else:\n",
        "            p_sqnorm = th.sum(p ** 2, dim=-1, keepdim=True)\n",
        "            d_p = d_p * ((1 - p_sqnorm) ** 2 / 4).expand_as(d_p)\n",
        "        return d_p\n",
        "\n",
        "\n",
        "class Distance(Function):\n",
        "    @staticmethod\n",
        "    def grad(x, v, sqnormx, sqnormv, sqdist, eps):\n",
        "        alpha = (1 - sqnormx)\n",
        "        beta = (1 - sqnormv)\n",
        "        z = 1 + 2 * sqdist / (alpha * beta)\n",
        "        a = ((sqnormv - 2 * th.sum(x * v, dim=-1) + 1) / th.pow(alpha, 2))\\\n",
        "            .unsqueeze(-1).expand_as(x)\n",
        "        a = a * x - v / alpha.unsqueeze(-1).expand_as(v)\n",
        "        z = th.sqrt(th.pow(z, 2) - 1)\n",
        "        z = th.clamp(z * beta, min=eps).unsqueeze(-1)\n",
        "        return 4 * a / z.expand_as(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, u, v, eps):\n",
        "        squnorm = th.clamp(th.sum(u * u, dim=-1), 0, 1 - eps)\n",
        "        sqvnorm = th.clamp(th.sum(v * v, dim=-1), 0, 1 - eps)\n",
        "        sqdist = th.sum(th.pow(u - v, 2), dim=-1)\n",
        "        ctx.eps = eps\n",
        "        ctx.save_for_backward(u, v, squnorm, sqvnorm, sqdist)\n",
        "        x = sqdist / ((1 - squnorm) * (1 - sqvnorm)) * 2 + 1\n",
        "        # arcosh\n",
        "        z = th.sqrt(th.pow(x, 2) - 1)\n",
        "        return th.log(x + z)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, g):\n",
        "        u, v, squnorm, sqvnorm, sqdist = ctx.saved_tensors\n",
        "        g = g.unsqueeze(-1)\n",
        "        gu = Distance.grad(u, v, squnorm, sqvnorm, sqdist, ctx.eps)\n",
        "        gv = Distance.grad(v, u, sqvnorm, squnorm, sqdist, ctx.eps)\n",
        "        return g.expand_as(gu) * gu, g.expand_as(gv) * gv, None\n",
        "\n",
        "\n",
        "###########################LorentzManifold################################\n",
        "\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "# Copyright (c) 2018-present, Facebook, Inc.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "\n",
        "import torch as th\n",
        "from torch.autograd import Function\n",
        "import numpy as np\n",
        "from torch.nn import Embedding\n",
        "\n",
        "\n",
        "class LorentzManifold(Manifold):\n",
        "    __slots__ = [\"eps\", \"_eps\", \"norm_clip\", \"max_norm\", \"debug\"]\n",
        "\n",
        "    def __init__(self, eps=1e-12, _eps=1e-5, norm_clip=1, max_norm=1e6,\n",
        "            debug=False, K=None, **kwargs):\n",
        "        self.eps = eps\n",
        "        self._eps = _eps\n",
        "        self.norm_clip = norm_clip\n",
        "        self.max_norm = max_norm\n",
        "        self.debug = debug\n",
        "        self.K = K\n",
        "        if K is not None:\n",
        "            self.inner_radius = 2 * self.K / (1 + np.sqrt(1 + 4 * self.K * self.K))\n",
        "\n",
        "    def allocate_lt(self, N, dim, sparse):\n",
        "        return Embedding(N, dim + 1, sparse=sparse)\n",
        "\n",
        "    def init_weights(self, w, irange=1e-5):\n",
        "        w.weight.data.uniform_(-irange, irange)\n",
        "        self.normalize(w.weight.data)\n",
        "\n",
        "    @staticmethod\n",
        "    def ldot(u, v, keepdim=False):\n",
        "        \"\"\"Lorentzian Scalar Product\"\"\"\n",
        "        uv = u * v\n",
        "        uv.narrow(-1, 0, 1).mul_(-1)\n",
        "        return th.sum(uv, dim=-1, keepdim=keepdim)\n",
        "\n",
        "    def to_poincare_ball(self, u):\n",
        "        x = u.clone()\n",
        "        d = x.size(-1) - 1\n",
        "        return x.narrow(-1, 1, d) / (x.narrow(-1, 0, 1) + 1)\n",
        "\n",
        "    def distance(self, u, v):\n",
        "        d = -LorentzDot.apply(u, v)\n",
        "        d.data.clamp_(min=1)\n",
        "        return acosh(d, self._eps)\n",
        "\n",
        "    def norm(self, u):\n",
        "        return th.sqrt(th.sum(th.pow(self.to_poincare_ball(u), 2), dim=-1))\n",
        "\n",
        "    def normalize(self, w):\n",
        "        \"\"\"Normalize vector such that it is located on the hyperboloid\"\"\"\n",
        "        d = w.size(-1) - 1\n",
        "        narrowed = w.narrow(-1, 1, d)\n",
        "        if self.max_norm:\n",
        "            narrowed.view(-1, d).renorm_(p=2, dim=0, maxnorm=self.max_norm)\n",
        "\n",
        "        if self.K is not None:\n",
        "            # Push embeddings outside of `inner_radius`\n",
        "            w0 = w.narrow(-1, 0, 1).squeeze()\n",
        "            wnrm = th.sqrt(th.sum(th.pow(narrowed, 2), dim=-1)) / (1 + w0)\n",
        "            scal = th.ones_like(wnrm)\n",
        "            ix = wnrm < (self.inner_radius + self._eps)\n",
        "            scal[ix] = (self.inner_radius + self._eps) / wnrm[ix]\n",
        "            narrowed.mul_(scal.unsqueeze(-1))\n",
        "\n",
        "        tmp = 1 + th.sum(th.pow(narrowed, 2), dim=-1, keepdim=True)\n",
        "        tmp.sqrt_()\n",
        "        w.narrow(-1, 0, 1).copy_(tmp)\n",
        "        return w\n",
        "\n",
        "    def normalize_tan(self, x_all, v_all):\n",
        "        d = v_all.size(1) - 1\n",
        "        x = x_all.narrow(1, 1, d)\n",
        "        xv = th.sum(x * v_all.narrow(1, 1, d), dim=1, keepdim=True)\n",
        "        tmp = 1 + th.sum(th.pow(x_all.narrow(1, 1, d), 2), dim=1, keepdim=True)\n",
        "        tmp.sqrt_().clamp_(min=self._eps)\n",
        "        v_all.narrow(1, 0, 1).copy_(xv / tmp)\n",
        "        return v_all\n",
        "\n",
        "    def rgrad(self, p, d_p):\n",
        "        \"\"\"Riemannian gradient for hyperboloid\"\"\"\n",
        "        if d_p.is_sparse:\n",
        "            u = d_p._values()\n",
        "            x = p.index_select(0, d_p._indices().squeeze())\n",
        "        else:\n",
        "            u = d_p\n",
        "            x = p\n",
        "        u.narrow(-1, 0, 1).mul_(-1)\n",
        "        u.addcmul_(self.ldot(x, u, keepdim=True).expand_as(x), x)\n",
        "        return d_p\n",
        "\n",
        "    def expm(self, p, d_p, lr=None, out=None, normalize=False):\n",
        "        \"\"\"Exponential map for hyperboloid\"\"\"\n",
        "        if out is None:\n",
        "            out = p\n",
        "        if d_p.is_sparse:\n",
        "            ix, d_val = d_p._indices().squeeze(), d_p._values()\n",
        "            # This pulls `ix` out of the original embedding table, which could\n",
        "            # be in a corrupted state.  normalize it to fix it back to the\n",
        "            # surface of the hyperboloid...\n",
        "            # TODO: we should only do the normalize if we know that we are\n",
        "            # training with multiple threads, otherwise this is a bit wasteful\n",
        "            p_val = self.normalize(p.index_select(0, ix))\n",
        "            ldv = self.ldot(d_val, d_val, keepdim=True)\n",
        "            if self.debug:\n",
        "                assert all(ldv > 0), \"Tangent norm must be greater 0\"\n",
        "                assert all(ldv == ldv), \"Tangent norm includes NaNs\"\n",
        "            nd_p = ldv.clamp_(min=0).sqrt_()\n",
        "            t = th.clamp(nd_p, max=self.norm_clip)\n",
        "            nd_p.clamp_(min=self.eps)\n",
        "            newp = (th.cosh(t) * p_val).addcdiv_(th.sinh(t) * d_val, nd_p)\n",
        "            if normalize:\n",
        "                newp = self.normalize(newp)\n",
        "            p.index_copy_(0, ix, newp)\n",
        "        else:\n",
        "            if lr is not None:\n",
        "                d_p.narrow(-1, 0, 1).mul_(-1)\n",
        "                d_p.addcmul_((self.ldot(p, d_p, keepdim=True)).expand_as(p), p)\n",
        "                d_p.mul_(-lr)\n",
        "            ldv = self.ldot(d_p, d_p, keepdim=True)\n",
        "            if self.debug:\n",
        "                assert all(ldv > 0), \"Tangent norm must be greater 0\"\n",
        "                assert all(ldv == ldv), \"Tangent norm includes NaNs\"\n",
        "            nd_p = ldv.clamp_(min=0).sqrt_()\n",
        "            t = th.clamp(nd_p, max=self.norm_clip)\n",
        "            nd_p.clamp_(min=self.eps)\n",
        "            newp = (th.cosh(t) * p).addcdiv_(th.sinh(t) * d_p, nd_p)\n",
        "            if normalize:\n",
        "                newp = self.normalize(newp)\n",
        "            p.copy_(newp)\n",
        "\n",
        "    def logm(self, x, y):\n",
        "        \"\"\"Logarithmic map on the Lorenz Manifold\"\"\"\n",
        "        xy = th.clamp(self.ldot(x, y).unsqueeze(-1), max=-1)\n",
        "        v = acosh(-xy, self.eps).div_(\n",
        "            th.clamp(th.sqrt(xy * xy - 1), min=self._eps)\n",
        "        ) * th.addcmul(y, xy, x)\n",
        "        return self.normalize_tan(x, v)\n",
        "\n",
        "    def ptransp(self, x, y, v, ix=None, out=None):\n",
        "        \"\"\"Parallel transport for hyperboloid\"\"\"\n",
        "        if ix is not None:\n",
        "            v_ = v\n",
        "            x_ = x.index_select(0, ix)\n",
        "            y_ = y.index_select(0, ix)\n",
        "        elif v.is_sparse:\n",
        "            ix, v_ = v._indices().squeeze(), v._values()\n",
        "            x_ = x.index_select(0, ix)\n",
        "            y_ = y.index_select(0, ix)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        xy = self.ldot(x_, y_, keepdim=True).expand_as(x_)\n",
        "        vy = self.ldot(v_, y_, keepdim=True).expand_as(x_)\n",
        "        vnew = v_ + vy / (1 - xy) * (x_ + y_)\n",
        "        if out is None:\n",
        "            return vnew\n",
        "        else:\n",
        "            out.index_copy_(0, ix, vnew)\n",
        "\n",
        "    def half_aperture(self, u):\n",
        "        eps = self.eps\n",
        "        d = u.size(-1) - 1\n",
        "        sqnu = th.sum(u.narrow(-1, 1, d) ** 2, dim=-1) / (1 + u.narrow(-1, 0, 1)\n",
        "            .squeeze(-1)) ** 2\n",
        "        sqnu.clamp_(min=0, max=1 - eps)\n",
        "        return th.asin((self.inner_radius * (1 - sqnu) / th.sqrt(sqnu))\n",
        "            .clamp(min=-1 + eps, max=1 - eps))\n",
        "\n",
        "    def angle_at_u(self, u, v):\n",
        "        uvldot = LorentzDot.apply(u, v)\n",
        "        u0 = u.narrow(-1, 0, 1).squeeze(-1)\n",
        "        num = th.add(v.narrow(-1, 0, 1).squeeze(-1), th.mul(u0, uvldot))\n",
        "        tmp = th.pow(uvldot, 2) - 1.\n",
        "        den = th.sqrt(th.pow(u0, 2) - 1.) * th.sqrt(tmp.clamp_(min=self.eps))\n",
        "        frac = th.div(num, den)\n",
        "        if self.debug and (frac != frac).any():\n",
        "            import ipdb; ipdb.set_trace()\n",
        "        frac.data.clamp_(min=-1 + self.eps, max=1 - self.eps)\n",
        "        ksi = frac.acos()\n",
        "        return ksi\n",
        "\n",
        "    def norm(self, u):\n",
        "        if isinstance(u, Embedding):\n",
        "            u = u.weight\n",
        "        d = u.size(-1) - 1\n",
        "        sqnu = th.sum(u.narrow(-1, 1, d) ** 2, dim=-1)\n",
        "        sqnu = sqnu / (1 + u.narrow(-1, 0, 1).squeeze(-1)) ** 2\n",
        "        return sqnu.sqrt()\n",
        "\n",
        "\n",
        "class LorentzDot(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, u, v):\n",
        "        ctx.save_for_backward(u, v)\n",
        "        return LorentzManifold.ldot(u, v)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, g):\n",
        "        u, v = ctx.saved_tensors\n",
        "        g = g.unsqueeze(-1).expand_as(u).clone()\n",
        "        g.narrow(-1, 0, 1).mul_(-1)\n",
        "        return g * v, g * u"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8bUhVOmVzYR"
      },
      "source": [
        "###Hype###\n",
        "\n",
        "Contains checkpoint code and all other util files (including data pre-processing, data splitting etc), as well as some mathematical operations (poisson kernel) etc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrPBLjT8V1nI"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "#!/usr/bin/env python3\n",
        "# Copyright (c) 2018-present, Facebook, Inc.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "\n",
        "\n",
        "####################checkpoint.py#######################\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import warnings\n",
        "from fvcore.common.file_io import PathManager\n",
        "\n",
        "\n",
        "def upgrade_state_dict(state):\n",
        "    '''\n",
        "    Used to upgrade old checkpoints to deal with breaking changes\n",
        "    '''\n",
        "    conf = state['conf']\n",
        "\n",
        "    # Previously we only had `-manifold`.  if this is an old checkpoint, then\n",
        "    # update the `conf` to use the same manifold and \"distance\" model...\n",
        "    if 'model' not in conf:\n",
        "        warnings.warn(\n",
        "            'Missing `model` field in checkpoint config.'\n",
        "            '  Assuming `distance`.'\n",
        "        )\n",
        "        conf['model'] = 'distance'\n",
        "    return state\n",
        "\n",
        "\n",
        "class LocalCheckpoint(object):\n",
        "    def __init__(self, path, include_in_all=None, start_fresh=False):\n",
        "        self.path = path\n",
        "        self.start_fresh = start_fresh\n",
        "        self.include_in_all = {} if include_in_all is None else include_in_all\n",
        "\n",
        "    def initialize(self, params):\n",
        "        if not self.start_fresh and os.path.isfile(self.path):\n",
        "            print(f'Loading checkpoint from {self.path}')\n",
        "            with PathManager.open(self.path, 'rb') as fin:\n",
        "                return torch.load(fin)\n",
        "        else:\n",
        "            return params\n",
        "\n",
        "    def load(self):\n",
        "        if os.path.isfile(self.path):\n",
        "            print(f'Loading checkpoint from {self.path}')\n",
        "            with PathManager.open(self.path, 'rb') as fin:\n",
        "                return torch.load(fin)\n",
        "        else:\n",
        "            print('not a valid path to load from')\n",
        "            raise NotImplemented\n",
        "\n",
        "    def save(self, params, tries=10):\n",
        "        try:\n",
        "            with PathManager.open(self.path, 'wb') as fout:\n",
        "                torch.save({**self.include_in_all, **params}, fout)\n",
        "        except Exception as err:\n",
        "            if tries > 0:\n",
        "                print(f'Exception while saving ({err})\\nRetrying ({tries})')\n",
        "                time.sleep(60)\n",
        "                self.save(params, tries=(tries - 1))\n",
        "            else:\n",
        "                print(\"Giving up on saving...\")\n",
        "\n",
        "\n",
        "\n",
        "###############common.py######################\n",
        "import torch as th\n",
        "from torch.autograd import Function\n",
        "\n",
        "\n",
        "class Acosh(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, eps):\n",
        "        z = th.sqrt(x * x - 1)\n",
        "        ctx.save_for_backward(z)\n",
        "        ctx.eps = eps\n",
        "        return th.log(x + z)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, g):\n",
        "        z, = ctx.saved_tensors\n",
        "        z = th.clamp(z, min=ctx.eps)\n",
        "        z = g / z\n",
        "        return z, None\n",
        "\n",
        "\n",
        "acosh = Acosh.apply\n",
        "\n",
        "#####################hyla_utils.py##################################\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "from sklearn.metrics import average_precision_score, accuracy_score, f1_score\n",
        "import os\n",
        "import pickle as pkl\n",
        "import sys\n",
        "import networkx as nx\n",
        "import torch.nn.functional as F\n",
        "import json\n",
        "from networkx.readwrite import json_graph\n",
        "import pdb\n",
        "from scipy.sparse.linalg import eigsh\n",
        "#from scipy.sparse.linalg.eigen.arpack import eigsh\n",
        "from scipy.sparse import csr_matrix\n",
        "import re\n",
        "from time import perf_counter\n",
        "import tabulate\n",
        "sys.setrecursionlimit(99999)\n",
        "\n",
        "\n",
        "def sample_boundary(n_Bs, d, cls):\n",
        "    if cls =='RandomUniform' or d>2:\n",
        "        pre_b = torch.randn(n_Bs, d)\n",
        "        b = pre_b/torch.norm(pre_b,dim=-1,keepdim=True)\n",
        "    elif cls == 'FixedUniform':\n",
        "        theta = torch.arange(0,2 * np.pi, 2*np.pi/n_Bs)\n",
        "        b = torch.stack([torch.cos(theta), torch.sin(theta)],1)\n",
        "    elif cls == 'RandomDisk':\n",
        "        theta = 2 * np.pi * torch.rand(n_Bs)\n",
        "        b = torch.stack([torch.cos(theta), torch.sin(theta)],1)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    return b\n",
        "\n",
        "def PoissonKernel(X, b):\n",
        "    X = X.view(X.size(0), 1, X.size(-1))\n",
        "    return (1 - torch.norm(X, 2, dim=-1)**2)/(torch.norm(X-b, 2, dim=-1)**2)\n",
        "#     return (1 - torch.sum(X * X, dim=-1))/torch.sum((X-b)**2,dim=-1)\n",
        "\n",
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float64)\n",
        "    indices = torch.from_numpy(\n",
        "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.DoubleTensor(indices, values, shape)\n",
        "\n",
        "\n",
        "def accuracy(output, labels):\n",
        "    preds = output.max(1)[1].type_as(labels)\n",
        "    correct = preds.eq(labels).double()\n",
        "    correct = correct.sum()\n",
        "    return correct / len(labels)\n",
        "\n",
        "def aug_normalized_adjacency(adj):\n",
        "    adj = adj + sp.eye(adj.shape[0])\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    row_sum = np.array(adj.sum(1))\n",
        "    d_inv_sqrt = np.power(row_sum, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    return d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt).tocoo()\n",
        "\n",
        "def add_self_loop(adj):\n",
        "    return adj + sp.eye(adj.shape[0])\n",
        "\n",
        "def sgc_precompute(adj, features, degree):\n",
        "    nonzero_perc = []\n",
        "#     assert degree > 0, 'invalid degree as 0'\n",
        "    if degree==0:\n",
        "        number_nonzero = (features != 0).sum().item()\n",
        "        percentage = number_nonzero*1.0/features.size(0)/features.size(1)*100.0\n",
        "        nonzero_perc.append(\"%.2f\" % percentage)\n",
        "        print('input order 0, return raw feature')\n",
        "        return features, nonzero_perc\n",
        "    for i in range(degree):\n",
        "        features = torch.spmm(adj, features)\n",
        "        number_nonzero = (features != 0).sum().item()\n",
        "        percentage = number_nonzero*1.0/features.size(0)/features.size(1)*100.0\n",
        "        nonzero_perc.append(\"%.2f\" % percentage)\n",
        "    return features, nonzero_perc\n",
        "\n",
        "\n",
        "def adj_compute(adj, degree):\n",
        "    new_adj = adj.to_dense().clone()\n",
        "    result = adj.to_dense().clone()\n",
        "    for i in range(degree):\n",
        "        result = torch.spmm(new_adj, result)\n",
        "    return result\n",
        "\n",
        "def acc_f1(output, labels, average='micro'):\n",
        "    preds = output.max(1)[1].type_as(labels)\n",
        "    if preds.is_cuda:\n",
        "        preds = preds.cpu()\n",
        "        labels = labels.cpu()\n",
        "    accuracy = accuracy_score(preds, labels)\n",
        "    f1 = f1_score(preds, labels, average=average)\n",
        "    return accuracy, f1\n",
        "\n",
        "def measure_tensor_size(a):\n",
        "    # return # MB\n",
        "    return a.element_size() * a.nelement() * 0.000001\n",
        "\n",
        "# ###################################################\n",
        "# data loading\n",
        "\n",
        "def load_data(args, datapath):\n",
        "    data = load_data_nc(args.dataset, args.use_feats, datapath, args.split_seed)\n",
        "    adj_n = aug_normalized_adjacency(data['adj_train'])\n",
        "    data['adj_train'] = sparse_mx_to_torch_sparse_tensor(adj_n)\n",
        "    data['features'] = sparse_mx_to_torch_sparse_tensor(data['features'])\n",
        "    return data\n",
        "\n",
        "# ############### FEATURES PROCESSING ####################################\n",
        "\n",
        "\n",
        "def process(adj, features, normalize_adj, normalize_feats):\n",
        "    if sp.isspmatrix(features):\n",
        "        features = np.array(features.todense())\n",
        "    if normalize_feats:\n",
        "        features = normalize(features)\n",
        "    features = torch.Tensor(features)\n",
        "    if normalize_adj:\n",
        "        adj = normalize(adj + sp.eye(adj.shape[0]))\n",
        "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
        "    return adj, features\n",
        "\n",
        "\n",
        "def normalize(mx):\n",
        "    \"\"\"Row-normalize sparse matrix.\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx\n",
        "\n",
        "\n",
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "    sparse_mx = sparse_mx.tocoo()\n",
        "    indices = torch.from_numpy(\n",
        "            np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64)\n",
        "    )\n",
        "    values = torch.Tensor(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)\n",
        "\n",
        "\n",
        "def augment(adj, features, normalize_feats=True):\n",
        "    deg = np.squeeze(np.sum(adj, axis=0).astype(int))\n",
        "    deg[deg > 5] = 5\n",
        "    deg_onehot = torch.tensor(np.eye(6)[deg], dtype=torch.float).squeeze()\n",
        "    const_f = torch.ones(features.size(0), 1)\n",
        "    features = torch.cat((features, deg_onehot, const_f), dim=1)\n",
        "    return features\n",
        "\n",
        "\n",
        "# ############### DATA SPLITS #####################################################\n",
        "\n",
        "\n",
        "def mask_edges(adj, val_prop, test_prop, seed):\n",
        "    np.random.seed(seed)  # get tp edges\n",
        "    x, y = sp.triu(adj).nonzero()\n",
        "    pos_edges = np.array(list(zip(x, y)))\n",
        "    np.random.shuffle(pos_edges)\n",
        "    # get tn edges\n",
        "    x, y = sp.triu(sp.csr_matrix(1. - adj.toarray())).nonzero()\n",
        "    neg_edges = np.array(list(zip(x, y)))\n",
        "    np.random.shuffle(neg_edges)\n",
        "\n",
        "    m_pos = len(pos_edges)\n",
        "    n_val = int(m_pos * val_prop)\n",
        "    n_test = int(m_pos * test_prop)\n",
        "    val_edges, test_edges, train_edges = pos_edges[:n_val], pos_edges[n_val:n_test + n_val], pos_edges[n_test + n_val:]\n",
        "    val_edges_false, test_edges_false = neg_edges[:n_val], neg_edges[n_val:n_test + n_val]\n",
        "    train_edges_false = np.concatenate([neg_edges, val_edges, test_edges], axis=0)\n",
        "    adj_train = sp.csr_matrix((np.ones(train_edges.shape[0]), (train_edges[:, 0], train_edges[:, 1])), shape=adj.shape)\n",
        "    adj_train = adj_train + adj_train.T\n",
        "    return adj_train, torch.LongTensor(train_edges), torch.LongTensor(train_edges_false), torch.LongTensor(val_edges), \\\n",
        "           torch.LongTensor(val_edges_false), torch.LongTensor(test_edges), torch.LongTensor(\n",
        "            test_edges_false)\n",
        "\n",
        "\n",
        "def split_data(labels, val_prop, test_prop, seed):\n",
        "    np.random.seed(seed)\n",
        "    nb_nodes = labels.shape[0]\n",
        "    all_idx = np.arange(nb_nodes)\n",
        "    pos_idx = labels.nonzero()[0]\n",
        "    neg_idx = (1. - labels).nonzero()[0]\n",
        "    np.random.shuffle(pos_idx)\n",
        "    np.random.shuffle(neg_idx)\n",
        "    pos_idx = pos_idx.tolist()\n",
        "    neg_idx = neg_idx.tolist()\n",
        "    nb_pos_neg = min(len(pos_idx), len(neg_idx))\n",
        "    nb_val = round(val_prop * nb_pos_neg)\n",
        "    nb_test = round(test_prop * nb_pos_neg)\n",
        "    idx_val_pos, idx_test_pos, idx_train_pos = pos_idx[:nb_val], pos_idx[nb_val:nb_val + nb_test], pos_idx[nb_val + nb_test:]\n",
        "    idx_val_neg, idx_test_neg, idx_train_neg = neg_idx[:nb_val], neg_idx[nb_val:nb_val + nb_test], neg_idx[nb_val + nb_test:]\n",
        "    return idx_val_pos + idx_val_neg, idx_test_pos + idx_test_neg, idx_train_pos + idx_train_neg\n",
        "\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "def split_data2(labels, val_prop, test_prop, seed):\n",
        "    np.random.seed(seed)\n",
        "    nb_nodes = labels.shape[0]\n",
        "    # print(\"nb_nodes \",labels.shape)\n",
        "    # print(np.unique(np.array(labels)))\n",
        "    all_idx = np.arange(nb_nodes)\n",
        "    # pos_idx = labels.nonzero()[0]\n",
        "    pos_idx = np.array([index for index, value in enumerate(labels) if value == 1])\n",
        "    count_dict = Counter(labels)\n",
        "    neg_idx = (1 - labels).nonzero()[0]\n",
        "    np.random.shuffle(pos_idx)\n",
        "    np.random.shuffle(neg_idx)\n",
        "    pos_idx = pos_idx.tolist()\n",
        "    neg_idx = neg_idx.tolist()\n",
        "    nb_pos_neg = min(len(pos_idx), len(neg_idx))\n",
        "    nb_val = round(val_prop * nb_pos_neg)\n",
        "    nb_test = round(test_prop * nb_pos_neg)\n",
        "    idx_val_pos, idx_test_pos, idx_train_pos = pos_idx[:nb_val], pos_idx[nb_val:nb_val + nb_test], pos_idx[\n",
        "                                                                                                   nb_val + nb_test:]\n",
        "    idx_val_neg, idx_test_neg, idx_train_neg = neg_idx[:nb_val], neg_idx[nb_val:nb_val + nb_test], neg_idx[\n",
        "                                                                                                    nb_val + nb_test:]\n",
        "    return idx_val_pos + idx_val_neg, idx_test_pos + idx_test_neg, idx_train_pos + idx_train_neg\n",
        "\n",
        "\n",
        "def bin_feat(feat, bins):\n",
        "    digitized = np.digitize(feat, bins)\n",
        "    return digitized - digitized.min()\n",
        "\n",
        "\n",
        "# ############### LINK PREDICTION DATA LOADERS ####################################\n",
        "\n",
        "\n",
        "def load_data_lp(dataset, use_feats, data_path):\n",
        "    if dataset in ['cora', 'pubmed']:\n",
        "        adj, features = load_citation_data(dataset, use_feats, data_path)[:2]\n",
        "    elif dataset == 'disease_lp':\n",
        "        adj, features = load_synthetic_data(dataset, use_feats, data_path)[:2]\n",
        "    elif dataset == 'airport':\n",
        "        adj, features = load_data_airport(dataset, data_path, return_label=False)\n",
        "    else:\n",
        "        raise FileNotFoundError('Dataset {} is not supported.'.format(dataset))\n",
        "    data = {'adj_train': adj, 'features': features}\n",
        "    return data\n",
        "\n",
        "\n",
        "# ############### NODE CLASSIFICATION DATA LOADERS ####################################\n",
        "\n",
        "\n",
        "def load_data_nc(dataset, use_feats, data_path, split_seed):\n",
        "    if dataset in ['cora', 'pubmed', 'citeseer']:\n",
        "        adj, features, labels, idx_train, idx_val, idx_test = load_citation_data(\n",
        "            dataset, use_feats, data_path, split_seed\n",
        "        )\n",
        "    else:\n",
        "        if dataset == 'disease_nc':\n",
        "            adj, features, labels = load_synthetic_data(dataset, use_feats, data_path)\n",
        "            val_prop, test_prop = 0.10, 0.60\n",
        "            idx_val, idx_test, idx_train = split_data(labels, val_prop, test_prop, seed=split_seed)\n",
        "        elif dataset == 'airport':\n",
        "            adj, features, labels = load_data_airport(dataset, data_path, return_label=True)\n",
        "            val_prop, test_prop = 0.05, 0.10\n",
        "            idx_val, idx_test, idx_train = split_data2(labels, val_prop, test_prop, seed=split_seed)\n",
        "            print(len(idx_train))\n",
        "            print(len(idx_val))\n",
        "            print(len(idx_test))\n",
        "        else:\n",
        "            raise FileNotFoundError('Dataset {} is not supported.'.format(dataset))\n",
        "    labels = torch.LongTensor(labels)\n",
        "    data = {'adj_train': adj, 'features': features, 'labels': labels, 'idx_train': idx_train, 'idx_val': idx_val, 'idx_test': idx_test}\n",
        "    return data\n",
        "\n",
        "\n",
        "# ############### DATASETS ####################################\n",
        "\n",
        "def loadRedditFromNPZ(dataset_dir):\n",
        "    adj = sp.load_npz(dataset_dir+\"reddit_adj.npz\")\n",
        "    data = np.load(dataset_dir+\"reddit.npz\")\n",
        "\n",
        "    return adj, data['feats'], data['y_train'], data['y_val'], data['y_test'], data['train_index'], data['val_index'], data['test_index']\n",
        "\n",
        "def load_reddit_data(data_path):\n",
        "    adj, features, y_train, y_val, y_test, train_index, val_index, test_index = loadRedditFromNPZ(data_path)\n",
        "    labels = np.zeros(adj.shape[0])\n",
        "    labels[train_index]  = y_train\n",
        "    labels[val_index]  = y_val\n",
        "    labels[test_index]  = y_test\n",
        "    adj = adj + adj.T # remove maybe?\n",
        "    train_adj = adj[train_index, :][:, train_index]\n",
        "    features = torch.tensor(np.array(features))\n",
        "    features = (features-features.mean(dim=0))/features.std(dim=0)\n",
        "\n",
        "    adj = aug_normalized_adjacency(adj)\n",
        "    adj = sparse_mx_to_torch_sparse_tensor(adj)#.float()\n",
        "    train_adj = aug_normalized_adjacency(train_adj)\n",
        "    train_adj = sparse_mx_to_torch_sparse_tensor(train_adj)#.float()\n",
        "    labels = torch.LongTensor(labels)\n",
        "\n",
        "    data = {'adj_all': adj, 'adj_train': train_adj, 'features': features, 'labels': labels, 'idx_train': train_index, 'idx_val': val_index, 'idx_test': test_index}\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_citation_data(dataset_str, use_feats, data_path, split_seed=None):\n",
        "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
        "    objects = []\n",
        "    for i in range(len(names)):\n",
        "        with open(os.path.join(data_path, \"ind.{}.{}\".format(dataset_str, names[i])), 'rb') as f:\n",
        "            if sys.version_info > (3, 0):\n",
        "                objects.append(pkl.load(f, encoding='latin1'))\n",
        "            else:\n",
        "                objects.append(pkl.load(f))\n",
        "\n",
        "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
        "    test_idx_reorder = parse_index_file(os.path.join(data_path, \"ind.{}.test.index\".format(dataset_str)))\n",
        "    test_idx_range = np.sort(test_idx_reorder)\n",
        "\n",
        "    if dataset_str == 'citeseer':\n",
        "        # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
        "        # Find isolated nodes, add them as zero-vecs into the right position\n",
        "        test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder)+1)\n",
        "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
        "        tx_extended[test_idx_range-min(test_idx_range), :] = tx\n",
        "        tx = tx_extended\n",
        "        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n",
        "        ty_extended[test_idx_range-min(test_idx_range), :] = ty\n",
        "        ty = ty_extended\n",
        "\n",
        "    features = sp.vstack((allx, tx)).tolil()\n",
        "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
        "\n",
        "    labels = np.vstack((ally, ty))\n",
        "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
        "    labels = np.argmax(labels, 1)\n",
        "\n",
        "    idx_test = test_idx_range.tolist()\n",
        "    idx_train = list(range(len(y)))\n",
        "    idx_val = range(len(y), len(y) + 500)\n",
        "\n",
        "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
        "    if not use_feats:\n",
        "        features = sp.eye(adj.shape[0])\n",
        "    print(len(idx_train))\n",
        "    print(len(idx_val))\n",
        "    print(len(idx_test))\n",
        "    return adj, features, labels, idx_train, idx_val, idx_test\n",
        "\n",
        "\n",
        "def parse_index_file(filename):\n",
        "    index = []\n",
        "    for line in open(filename):\n",
        "        index.append(int(line.strip()))\n",
        "    return index\n",
        "\n",
        "\n",
        "def load_synthetic_data(dataset_str, use_feats, data_path):\n",
        "    object_to_idx = {}\n",
        "    idx_counter = 0\n",
        "    edges = []\n",
        "    with open(os.path.join(data_path, \"{}.edges.csv\".format(dataset_str)), 'r') as f:\n",
        "        all_edges = f.readlines()\n",
        "    for line in all_edges:\n",
        "        n1, n2 = line.rstrip().split(',')\n",
        "        if n1 in object_to_idx:\n",
        "            i = object_to_idx[n1]\n",
        "        else:\n",
        "            i = idx_counter\n",
        "            object_to_idx[n1] = i\n",
        "            idx_counter += 1\n",
        "        if n2 in object_to_idx:\n",
        "            j = object_to_idx[n2]\n",
        "        else:\n",
        "            j = idx_counter\n",
        "            object_to_idx[n2] = j\n",
        "            idx_counter += 1\n",
        "        edges.append((i, j))\n",
        "    adj = np.zeros((len(object_to_idx), len(object_to_idx)))\n",
        "    for i, j in edges:\n",
        "        adj[i, j] = 1.  # comment this line for directed adjacency matrix\n",
        "        adj[j, i] = 1.\n",
        "    if use_feats:\n",
        "        features = sp.load_npz(os.path.join(data_path, \"{}.feats.npz\".format(dataset_str)))\n",
        "    else:\n",
        "        features = sp.eye(adj.shape[0])\n",
        "    labels = np.load(os.path.join(data_path, \"{}.labels.npy\".format(dataset_str)))\n",
        "    return sp.csr_matrix(adj), features, labels\n",
        "\n",
        "\n",
        "def load_data_airport(dataset_str, data_path, return_label=False,num_nodes=3188):\n",
        "    graph = pkl.load(open(os.path.join(data_path, dataset_str + '.p'), 'rb'))\n",
        "    #graph = pkl.load(open(data_path, 'rb'))\n",
        "    selected_nodes=list(graph.nodes)[:num_nodes]\n",
        "    subgraph=graph.subgraph(selected_nodes)\n",
        "    adj = nx.adjacency_matrix(subgraph)\n",
        "    #features = np.array([graph.node[u]['feat'] for u in graph.nodes()])\n",
        "    features = np.array([subgraph.nodes[u]['feat'] for u in subgraph.nodes()])\n",
        "    if return_label:\n",
        "        label_idx = 4\n",
        "        labels = features[:, label_idx]\n",
        "        features = features[:, :label_idx]\n",
        "        labels = bin_feat(labels, bins=[7.0/7, 8.0/7, 9.0/7])\n",
        "        return sp.csr_matrix(adj), sp.csr_matrix(features), labels\n",
        "    else:\n",
        "        return sp.csr_matrix(adj), sp.csr_matrix(features)\n",
        "\n",
        "# ############### Loading ppi ####################################\n",
        "# adapted from PetarV/GAT\n",
        "def run_dfs(adj, msk, u, ind, nb_nodes):\n",
        "    if msk[u] == -1:\n",
        "        msk[u] = ind\n",
        "        #for v in range(nb_nodes):\n",
        "        for v in adj[u,:].nonzero()[1]:\n",
        "            #if adj[u,v]== 1:\n",
        "            run_dfs(adj, msk, v, ind, nb_nodes)\n",
        "\n",
        "def dfs_split(adj):\n",
        "    # Assume adj is of shape [nb_nodes, nb_nodes]\n",
        "    nb_nodes = adj.shape[0]\n",
        "    ret = np.full(nb_nodes, -1, dtype=np.int32)\n",
        "\n",
        "    graph_id = 0\n",
        "\n",
        "    for i in range(nb_nodes):\n",
        "        if ret[i] == -1:\n",
        "            run_dfs(adj, ret, i, graph_id, nb_nodes)\n",
        "            graph_id += 1\n",
        "\n",
        "    return ret\n",
        "\n",
        "def test(adj, mapping):\n",
        "    nb_nodes = adj.shape[0]\n",
        "    for i in range(nb_nodes):\n",
        "        #for j in range(nb_nodes):\n",
        "        for j in adj[i, :].nonzero()[1]:\n",
        "            if mapping[i] != mapping[j]:\n",
        "              #  if adj[i,j] == 1:\n",
        "                 return False\n",
        "    return True\n",
        "\n",
        "def find_split(adj, mapping, ds_label):\n",
        "    nb_nodes = adj.shape[0]\n",
        "    dict_splits={}\n",
        "    for i in range(nb_nodes):\n",
        "        #for j in range(nb_nodes):\n",
        "        for j in adj[i, :].nonzero()[1]:\n",
        "            if mapping[i]==0 or mapping[j]==0:\n",
        "                dict_splits[0]=None\n",
        "            elif mapping[i] == mapping[j]:\n",
        "                if ds_label[i]['val'] == ds_label[j]['val'] and ds_label[i]['test'] == ds_label[j]['test']:\n",
        "\n",
        "                    if mapping[i] not in dict_splits.keys():\n",
        "                        if ds_label[i]['val']:\n",
        "                            dict_splits[mapping[i]] = 'val'\n",
        "\n",
        "                        elif ds_label[i]['test']:\n",
        "                            dict_splits[mapping[i]]='test'\n",
        "\n",
        "                        else:\n",
        "                            dict_splits[mapping[i]] = 'train'\n",
        "\n",
        "                    else:\n",
        "                        if ds_label[i]['test']:\n",
        "                            ind_label='test'\n",
        "                        elif ds_label[i]['val']:\n",
        "                            ind_label='val'\n",
        "                        else:\n",
        "                            ind_label='train'\n",
        "                        if dict_splits[mapping[i]]!= ind_label:\n",
        "                            print ('inconsistent labels within a graph exiting!!!')\n",
        "                            return None\n",
        "                else:\n",
        "                    print ('label of both nodes different, exiting!!')\n",
        "                    return None\n",
        "    return dict_splits\n",
        "\n",
        "def load_ppi(data_path):\n",
        "\n",
        "    print ('Loading G...')\n",
        "    with open(data_path + 'ppi-G.json') as jsonfile:\n",
        "        g_data = json.load(jsonfile)\n",
        "    # print (len(g_data))\n",
        "    G = json_graph.node_link_graph(g_data)\n",
        "\n",
        "    #Extracting adjacency matrix\n",
        "    adj=nx.adjacency_matrix(G)\n",
        "\n",
        "    prev_key=''\n",
        "    for key, value in g_data.items():\n",
        "        if prev_key!=key:\n",
        "            # print (key)\n",
        "            prev_key=key\n",
        "\n",
        "    # print ('Loading id_map...')\n",
        "    with open(data_path + 'ppi-id_map.json') as jsonfile:\n",
        "        id_map = json.load(jsonfile)\n",
        "    # print (len(id_map))\n",
        "\n",
        "    id_map = {int(k):int(v) for k,v in id_map.items()}\n",
        "    for key, value in id_map.items():\n",
        "        id_map[key]=[value]\n",
        "    # print (len(id_map))\n",
        "\n",
        "    print ('Loading features...')\n",
        "    features_=np.load(data_path + 'ppi-feats.npy')\n",
        "    # print (features_.shape)\n",
        "\n",
        "    #standarizing features\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "    train_ids = np.array([id_map[n] for n in G.nodes() if not G.node[n]['val'] and not G.node[n]['test']])\n",
        "    train_feats = features_[train_ids[:,0]]\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(train_feats)\n",
        "    features_ = scaler.transform(features_)\n",
        "\n",
        "    features = sp.csr_matrix(features_).tolil()\n",
        "\n",
        "\n",
        "    print ('Loading class_map...')\n",
        "    class_map = {}\n",
        "    with open(data_path + 'ppi-class_map.json') as jsonfile:\n",
        "        class_map = json.load(jsonfile)\n",
        "    # print (len(class_map))\n",
        "\n",
        "    #pdb.set_trace()\n",
        "    #Split graph into sub-graphs\n",
        "    # print ('Splitting graph...')\n",
        "    splits=dfs_split(adj)\n",
        "\n",
        "    #Rearrange sub-graph index and append sub-graphs with 1 or 2 nodes to bigger sub-graphs\n",
        "    # print ('Re-arranging sub-graph IDs...')\n",
        "    list_splits=splits.tolist()\n",
        "    group_inc=1\n",
        "\n",
        "    for i in range(np.max(list_splits)+1):\n",
        "        if list_splits.count(i)>=3:\n",
        "            splits[np.array(list_splits) == i] =group_inc\n",
        "            group_inc+=1\n",
        "        else:\n",
        "            #splits[np.array(list_splits) == i] = 0\n",
        "            ind_nodes=np.argwhere(np.array(list_splits) == i)\n",
        "            ind_nodes=ind_nodes[:,0].tolist()\n",
        "            split=None\n",
        "\n",
        "            for ind_node in ind_nodes:\n",
        "                if g_data['nodes'][ind_node]['val']:\n",
        "                    if split is None or split=='val':\n",
        "                        splits[np.array(list_splits) == i] = 21\n",
        "                        split='val'\n",
        "                    else:\n",
        "                        raise ValueError('new node is VAL but previously was {}'.format(split))\n",
        "                elif g_data['nodes'][ind_node]['test']:\n",
        "                    if split is None or split=='test':\n",
        "                        splits[np.array(list_splits) == i] = 23\n",
        "                        split='test'\n",
        "                    else:\n",
        "                        raise ValueError('new node is TEST but previously was {}'.format(split))\n",
        "                else:\n",
        "                    if split is None or split == 'train':\n",
        "                        splits[np.array(list_splits) == i] = 1\n",
        "                        split='train'\n",
        "                    else:\n",
        "                        pdb.set_trace()\n",
        "                        raise ValueError('new node is TRAIN but previously was {}'.format(split))\n",
        "\n",
        "    #counting number of nodes per sub-graph\n",
        "    list_splits=splits.tolist()\n",
        "    nodes_per_graph=[]\n",
        "    for i in range(1,np.max(list_splits) + 1):\n",
        "        nodes_per_graph.append(list_splits.count(i))\n",
        "\n",
        "    #Splitting adj matrix into sub-graphs\n",
        "    subgraph_nodes=np.max(nodes_per_graph)\n",
        "    adj_sub=np.empty((len(nodes_per_graph), subgraph_nodes, subgraph_nodes))\n",
        "    feat_sub = np.empty((len(nodes_per_graph), subgraph_nodes, features.shape[1]))\n",
        "    labels_sub = np.empty((len(nodes_per_graph), subgraph_nodes, 121))\n",
        "\n",
        "    for i in range(1, np.max(list_splits) + 1):\n",
        "        #Creating same size sub-graphs\n",
        "        indexes = np.where(splits == i)[0]\n",
        "        subgraph_=adj[indexes,:][:,indexes]\n",
        "\n",
        "        if subgraph_.shape[0]<subgraph_nodes or subgraph_.shape[1]<subgraph_nodes:\n",
        "            subgraph=np.identity(subgraph_nodes)\n",
        "            feats=np.zeros([subgraph_nodes, features.shape[1]])\n",
        "            labels=np.zeros([subgraph_nodes,121])\n",
        "            #adj\n",
        "            subgraph = sp.csr_matrix(subgraph).tolil()\n",
        "            subgraph[0:subgraph_.shape[0],0:subgraph_.shape[1]]=subgraph_\n",
        "            adj_sub[i-1,:,:]=subgraph.todense()\n",
        "            #feats\n",
        "            feats[0:len(indexes)]=features[indexes,:].todense()\n",
        "            feat_sub[i-1,:,:]=feats\n",
        "            #labels\n",
        "            for j,node in enumerate(indexes):\n",
        "                labels[j,:]=np.array(class_map[str(node)])\n",
        "            labels[indexes.shape[0]:subgraph_nodes,:]=np.zeros([121])\n",
        "            labels_sub[i - 1, :, :] = labels\n",
        "\n",
        "        else:\n",
        "            adj_sub[i - 1, :, :] = subgraph_.todense()\n",
        "            feat_sub[i - 1, :, :]=features[indexes,:].todense()\n",
        "            for j,node in enumerate(indexes):\n",
        "                labels[j,:]=np.array(class_map[str(node)])\n",
        "            labels_sub[i-1, :, :] = labels\n",
        "\n",
        "    # Get relation between id sub-graph and tran,val or test set\n",
        "    dict_splits = find_split(adj, splits, g_data['nodes'])\n",
        "\n",
        "    # Testing if sub graphs are isolated\n",
        "    # print ('Are sub-graphs isolated?')\n",
        "    # print (test(adj, splits))\n",
        "\n",
        "    #Splitting tensors into train,val and test\n",
        "    train_split=[]\n",
        "    val_split=[]\n",
        "    test_split=[]\n",
        "    for key, value in dict_splits.items():\n",
        "        if dict_splits[key]=='train':\n",
        "            train_split.append(int(key)-1)\n",
        "        elif dict_splits[key] == 'val':\n",
        "            val_split.append(int(key)-1)\n",
        "        elif dict_splits[key] == 'test':\n",
        "            test_split.append(int(key)-1)\n",
        "\n",
        "    train_adj=adj_sub[train_split,:,:]\n",
        "    val_adj=adj_sub[val_split,:,:]\n",
        "    test_adj=adj_sub[test_split,:,:]\n",
        "\n",
        "    train_feat=feat_sub[train_split,:,:]\n",
        "    val_feat = feat_sub[val_split, :, :]\n",
        "    test_feat = feat_sub[test_split, :, :]\n",
        "\n",
        "    train_labels = labels_sub[train_split, :, :]\n",
        "    val_labels = labels_sub[val_split, :, :]\n",
        "    test_labels = labels_sub[test_split, :, :]\n",
        "\n",
        "    train_nodes=np.array(nodes_per_graph[train_split[0]:train_split[-1]+1])\n",
        "    val_nodes = np.array(nodes_per_graph[val_split[0]:val_split[-1]+1])\n",
        "    test_nodes = np.array(nodes_per_graph[test_split[0]:test_split[-1]+1])\n",
        "\n",
        "\n",
        "    #Masks with ones\n",
        "\n",
        "    tr_msk = np.zeros((len(nodes_per_graph[train_split[0]:train_split[-1]+1]), subgraph_nodes))\n",
        "    vl_msk = np.zeros((len(nodes_per_graph[val_split[0]:val_split[-1] + 1]), subgraph_nodes))\n",
        "    ts_msk = np.zeros((len(nodes_per_graph[test_split[0]:test_split[-1]+1]), subgraph_nodes))\n",
        "\n",
        "    for i in range(len(train_nodes)):\n",
        "        for j in range(train_nodes[i]):\n",
        "            tr_msk[i][j] = 1\n",
        "\n",
        "    for i in range(len(val_nodes)):\n",
        "        for j in range(val_nodes[i]):\n",
        "            vl_msk[i][j] = 1\n",
        "\n",
        "    for i in range(len(test_nodes)):\n",
        "        for j in range(test_nodes[i]):\n",
        "            ts_msk[i][j] = 1\n",
        "\n",
        "    train_adj_list = []\n",
        "    val_adj_list = []\n",
        "    test_adj_list = []\n",
        "    for i in range(train_adj.shape[0]):\n",
        "        adj = sp.coo_matrix(train_adj[i])\n",
        "        adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "        tmp = aug_normalized_adjacency(adj)\n",
        "        train_adj_list.append(sparse_mx_to_torch_sparse_tensor(tmp))\n",
        "    for i in range(val_adj.shape[0]):\n",
        "        adj = sp.coo_matrix(val_adj[i])\n",
        "        adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "        tmp = aug_normalized_adjacency(adj)\n",
        "        val_adj_list.append(sparse_mx_to_torch_sparse_tensor(tmp))\n",
        "        adj = sp.coo_matrix(test_adj[i])\n",
        "        adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "        tmp = aug_normalized_adjacency(adj)\n",
        "        test_adj_list.append(sparse_mx_to_torch_sparse_tensor(tmp))\n",
        "\n",
        "    train_feat = torch.tensor(train_feat)\n",
        "    val_feat = torch.tensor(val_feat)\n",
        "    test_feat = torch.tensor(test_feat)\n",
        "\n",
        "    train_labels = torch.tensor(train_labels)\n",
        "    val_labels = torch.tensor(val_labels)\n",
        "    test_labels = torch.tensor(test_labels)\n",
        "\n",
        "    tr_msk = torch.LongTensor(tr_msk)\n",
        "    vl_msk = torch.LongTensor(vl_msk)\n",
        "    ts_msk = torch.LongTensor(ts_msk)\n",
        "\n",
        "    return train_adj_list,val_adj_list,test_adj_list,train_feat,val_feat,test_feat,train_labels,val_labels, test_labels, train_nodes, val_nodes, test_nodes\n",
        "\n",
        "# ############### Loading for TextHyLa ####################################\n",
        "# adapted from Tiiiger/SGC\n",
        "def parse_index_file(filename):\n",
        "    \"\"\"Parse index file.\"\"\"\n",
        "    index = []\n",
        "    for line in open(filename):\n",
        "        index.append(int(line.strip()))\n",
        "    return index\n",
        "\n",
        "def load_corpus(data_dir, dataset_str, inductive=False):\n",
        "    \"\"\"\n",
        "    Loads input corpus from text/data directory\n",
        "\n",
        "    ind.dataset_str.x => the feature vectors of the training docs as scipy.sparse.csr.csr_matrix object;\n",
        "    ind.dataset_str.tx => the feature vectors of the test docs as scipy.sparse.csr.csr_matrix object;\n",
        "    ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training docs/words\n",
        "        (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\n",
        "    ind.dataset_str.y => the one-hot labels of the labeled training docs as numpy.ndarray object;\n",
        "    ind.dataset_str.ty => the one-hot labels of the test docs as numpy.ndarray object;\n",
        "    ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n",
        "    ind.dataset_str.adj => adjacency matrix of word/doc nodes as scipy.sparse.csr.csr_matrix object;\n",
        "    ind.dataset_str.train.index => the indices of training docs in original doc list.\n",
        "\n",
        "    All objects above must be saved using python pickle module.\n",
        "\n",
        "    :param dataset_str: Dataset name\n",
        "    :return: All data input files loaded (as well the training/test data).\n",
        "    \"\"\"\n",
        "    index_dict = {}\n",
        "    label_dict = {}\n",
        "    phases = [\"train\", \"val\", \"test\"]\n",
        "    objects = []\n",
        "    def load_pkl(path):\n",
        "        with open(path.format(dataset_str, p), 'rb') as f:\n",
        "            if sys.version_info > (3, 0):\n",
        "                return pkl.load(f, encoding='latin1')\n",
        "            else:\n",
        "                return pkl.load(f)\n",
        "\n",
        "    for p in phases:\n",
        "        index_dict[p] = load_pkl(\"{}/ind.{}.{}.x\".format(data_dir, dataset_str, p))\n",
        "        label_dict[p] = load_pkl(\"{}/ind.{}.{}.y\".format(data_dir, dataset_str, p))\n",
        "\n",
        "    if inductive:\n",
        "        adj = load_pkl(\"{}/ind.{}.B.adj\".format(data_dir, dataset_str))\n",
        "        adj = adj.astype(np.float32)\n",
        "    else:\n",
        "        adj = load_pkl(\"{}/ind.{}.BCD.adj\".format(data_dir, dataset_str))\n",
        "        adj = adj.astype(np.float32)\n",
        "        adj = aug_normalized_adjacency(adj)\n",
        "\n",
        "    return adj, index_dict, label_dict\n",
        "\n",
        "def aug_normalized_adjacency(adj):\n",
        "    adj = adj + sp.eye(adj.shape[0])\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    row_sum = np.array(adj.sum(1))\n",
        "    d_inv_sqrt = np.power(row_sum, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    return d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt).tocoo()\n",
        "\n",
        "def loadWord2Vec(filename):\n",
        "    \"\"\"Read Word Vectors\"\"\"\n",
        "    vocab = []\n",
        "    embd = []\n",
        "    word_vector_map = {}\n",
        "    file = open(filename, 'r')\n",
        "    for line in file.readlines():\n",
        "        row = line.strip().split(' ')\n",
        "        if(len(row) > 2):\n",
        "            vocab.append(row[0])\n",
        "            vector = row[1:]\n",
        "            length = len(vector)\n",
        "            for i in range(length):\n",
        "                vector[i] = float(vector[i])\n",
        "            embd.append(vector)\n",
        "            word_vector_map[row[0]] = vector\n",
        "    print('Loaded Word Vectors!')\n",
        "    file.close()\n",
        "    return vocab, embd, word_vector_map\n",
        "\n",
        "def clean_str(string):\n",
        "    string = re.sub(r'[?|$|.|!]',r'',string)\n",
        "    string = re.sub(r'[^a-zA-Z0-9 ]',r'',string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" , \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
        "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
        "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    return string.strip().lower()\n",
        "\n",
        "def sparse_to_torch_sparse(sparse_mx, device='cuda'):\n",
        "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(\n",
        "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    if device == 'cuda':\n",
        "        indices = indices.cuda()\n",
        "        values = torch.from_numpy(sparse_mx.data).cuda()\n",
        "        shape = torch.Size(sparse_mx.shape)\n",
        "        adj = torch.cuda.sparse.FloatTensor(indices, values, shape)\n",
        "    elif device == 'cpu':\n",
        "        values = torch.from_numpy(sparse_mx.data)\n",
        "        shape = torch.Size(sparse_mx.shape)\n",
        "        adj = torch.sparse.FloatTensor(indices, values, shape)\n",
        "    return adj\n",
        "\n",
        "def sparse_to_torch_dense(sparse, device='cuda'):\n",
        "    dense = sparse.todense().astype(np.float32)\n",
        "    torch_dense = torch.from_numpy(dense).to(device=device)\n",
        "    return torch_dense\n",
        "\n",
        "def sgc_precompute_text(adj, features, degree, index_dict):\n",
        "#     assert degree==1, \"Only supporting degree 2 now\"\n",
        "    assert degree > 0, 'invalid degree as 0'\n",
        "    feat_dict = {}\n",
        "    start = perf_counter()\n",
        "    train_feats = features[:, index_dict[\"train\"]]#.cuda()\n",
        "    #     nonzero_perc = []\n",
        "    for i in range(degree):\n",
        "        train_feats = torch.spmm(adj, train_feats)\n",
        "#         number_nonzero = (features != 0).sum().item()\n",
        "#         percentage = number_nonzero*1.0/features.size(0)/features.size(1)*100.0\n",
        "#         nonzero_perc.append(\"%.2f\" % percentage)\n",
        "    train_feats = train_feats.t()\n",
        "    train_feats_max, _ = train_feats.max(dim=0, keepdim=True)\n",
        "    train_feats_min, _ = train_feats.min(dim=0, keepdim=True)\n",
        "    train_feats_range = train_feats_max-train_feats_min\n",
        "    useful_features_dim = train_feats_range.squeeze().gt(0).nonzero().squeeze()\n",
        "    train_feats = train_feats[:, useful_features_dim]\n",
        "    train_feats_range = train_feats_range[:, useful_features_dim]\n",
        "    train_feats_min = train_feats_min[:, useful_features_dim]\n",
        "    train_feats = (train_feats-train_feats_min)/train_feats_range\n",
        "    feat_dict[\"train\"] = train_feats.double()\n",
        "    for phase in [\"test\", \"val\"]:\n",
        "        feats = features[:, index_dict[phase]]#.cuda()\n",
        "        feats = torch.spmm(adj, feats).t()\n",
        "        feats = feats[:, useful_features_dim]\n",
        "        feat_dict[phase] = ((feats-train_feats_min)/train_feats_range).cpu().double() # adj is symmetric!\n",
        "    precompute_time = perf_counter()-start\n",
        "    return feat_dict, precompute_time\n",
        "\n",
        "def sgc_precompute_text_v1(adj, features, degree, index_dict):\n",
        "    assert degree > 0, 'invalid degree as 0'\n",
        "    feat_dict = {}\n",
        "    start = perf_counter()\n",
        "    for i in range(degree):\n",
        "        features = torch.spmm(adj, features)\n",
        "    train_feats = features[index_dict[\"train\"], :].double()\n",
        "    train_feats_max, _ = train_feats.max(dim=0, keepdim=True)\n",
        "    train_feats_min, _ = train_feats.min(dim=0, keepdim=True)\n",
        "    train_feats_range = train_feats_max-train_feats_min\n",
        "    useful_features_dim = train_feats_range.squeeze().gt(0).nonzero().squeeze()\n",
        "    train_feats = train_feats[:, useful_features_dim]\n",
        "    train_feats_range = train_feats_range[:, useful_features_dim]\n",
        "    train_feats_min = train_feats_min[:, useful_features_dim]\n",
        "    feat_dict[\"train\"] = (train_feats-train_feats_min)/train_feats_range\n",
        "    for phase in [\"test\", \"val\"]:\n",
        "        feats = features[index_dict[phase], :].double()\n",
        "        feats = feats[:, useful_features_dim]\n",
        "        feat_dict[phase] = ((feats-train_feats_min)/train_feats_range).cpu() # adj is symmetric!\n",
        "    precompute_time = perf_counter()-start\n",
        "    return feat_dict, precompute_time\n",
        "\n",
        "def set_seed(seed, cuda):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if cuda: torch.cuda.manual_seed(seed)\n",
        "\n",
        "def print_table(values, columns, epoch):\n",
        "    table = tabulate.tabulate([values], columns, tablefmt='simple', floatfmt='8.4f')\n",
        "    if epoch % 40 == 0:\n",
        "        table = table.split('\\n')\n",
        "        table = '\\n'.join([table[1]] + table)\n",
        "    else:\n",
        "        table = table.split('\\n')[2]\n",
        "    print(table)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeS_qEwKY9HR"
      },
      "source": [
        "###RSGD/SGD/PGD optimizer###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QAGaR77ZAzY"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "#!/usr/bin/env python3\n",
        "# Copyright (c) 2018-present, Facebook, Inc.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "import tensorly as tl\n",
        "\n",
        "class RiemannianSGD(Optimizer):\n",
        "    r\"\"\"Riemannian stochastic gradient descent.\n",
        "\n",
        "    Args:\n",
        "        rgrad (Function): Function to compute the Riemannian gradient\n",
        "           from the Euclidean gradient\n",
        "        retraction (Function): Function to update the retraction\n",
        "           of the Riemannian gradient\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            params,\n",
        "            lr=required,\n",
        "            rgrad=required,\n",
        "            expm=required,\n",
        "    ):\n",
        "        defaults = {\n",
        "            'lr': lr,\n",
        "            'rgrad': rgrad,\n",
        "            'expm': expm,\n",
        "        }\n",
        "        super(RiemannianSGD, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self, lr=None, counts=None, **kwargs):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "\n",
        "        Arguments:\n",
        "            lr (float, optional): learning rate for the current update.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                lr = lr or group['lr']\n",
        "                rgrad = group['rgrad']\n",
        "                expm = group['expm']\n",
        "\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                d_p = p.grad.data\n",
        "                # make sure we have no duplicates in sparse tensor\n",
        "                if d_p.is_sparse:\n",
        "                    d_p = d_p.coalesce()\n",
        "                d_p = rgrad(p.data, d_p)\n",
        "                d_p.mul_(-lr)\n",
        "                expm(p.data, d_p)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "from torch.optim.sgd import SGD\n",
        "from torch.optim.optimizer import required\n",
        "from torch.optim import Optimizer\n",
        "import torch\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "\n",
        "class PGD(Optimizer):\n",
        "    \"\"\"Proximal gradient descent.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    params : iterable\n",
        "        iterable of parameters to optimize or dicts defining parameter groups\n",
        "    proxs : iterable\n",
        "        iterable of proximal operators\n",
        "    alpha : iterable\n",
        "        iterable of coefficients for proximal gradient descent\n",
        "    lr : float\n",
        "        learning rate\n",
        "    momentum : float\n",
        "        momentum factor (default: 0)\n",
        "    weight_decay : float\n",
        "        weight decay (L2 penalty) (default: 0)\n",
        "    dampening : float\n",
        "        dampening for momentum (default: 0)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, proxs, alphas, lr=required, momentum=0, dampening=0, weight_decay=0):\n",
        "        defaults = dict(lr=lr, momentum=0, dampening=0,\n",
        "                        weight_decay=0, nesterov=False)\n",
        "\n",
        "\n",
        "        super(PGD, self).__init__(params, defaults)\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('proxs', proxs)\n",
        "            group.setdefault('alphas', alphas)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(PGD, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('nesterov', False)\n",
        "            group.setdefault('proxs', proxs)\n",
        "            group.setdefault('alphas', alphas)\n",
        "\n",
        "    def step(self, delta=0, closure=None):\n",
        "         for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            weight_decay = group['weight_decay']\n",
        "            momentum = group['momentum']\n",
        "            dampening = group['dampening']\n",
        "            nesterov = group['nesterov']\n",
        "            proxs = group['proxs']\n",
        "            alphas = group['alphas']\n",
        "\n",
        "            # apply the proximal operator to each parameter in a group\n",
        "            for param in group['params']:\n",
        "                for prox_operator, alpha in zip(proxs, alphas):\n",
        "                    # param.data.add_(lr, -param.grad.data)\n",
        "                    # param.data.add_(delta)\n",
        "                    param.data = prox_operator(param.data, alpha=alpha*lr)\n",
        "\n",
        "\n",
        "class ProxOperators():\n",
        "    \"\"\"Proximal Operators.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.nuclear_norm = None\n",
        "\n",
        "    def prox_l1(self, data, alpha):\n",
        "        \"\"\"Proximal operator for l1 norm.\n",
        "        \"\"\"\n",
        "        data = torch.mul(torch.sign(data), torch.clamp(torch.abs(data)-alpha, min=0))\n",
        "        return data\n",
        "\n",
        "    def prox_nuclear(self, data, alpha):\n",
        "        \"\"\"Proximal operator for nuclear norm (trace norm).\n",
        "        \"\"\"\n",
        "        device = data.device\n",
        "        U, S, V = np.linalg.svd(data.cpu())\n",
        "        U, S, V = torch.FloatTensor(U).to(device), torch.FloatTensor(S).to(device), torch.FloatTensor(V).to(device)\n",
        "        self.nuclear_norm = S.sum()\n",
        "        # print(\"nuclear norm: %.4f\" % self.nuclear_norm)\n",
        "\n",
        "        diag_S = torch.diag(torch.clamp(S-alpha, min=0))\n",
        "        return torch.matmul(torch.matmul(U, diag_S), V)\n",
        "\n",
        "    def prox_nuclear_truncated_2(self, data, alpha, k=50):\n",
        "        device = data.device\n",
        "        tl.set_backend('pytorch')\n",
        "        U, S, V = tl.truncated_svd(data.cpu(), n_eigenvecs=k)\n",
        "        U, S, V = torch.FloatTensor(U).to(device), torch.FloatTensor(S).to(device), torch.FloatTensor(V).to(device)\n",
        "        self.nuclear_norm = S.sum()\n",
        "        # print(\"nuclear norm: %.4f\" % self.nuclear_norm)\n",
        "\n",
        "        S = torch.clamp(S-alpha, min=0)\n",
        "\n",
        "        # diag_S = torch.diag(torch.clamp(S-alpha, min=0))\n",
        "        # U = torch.spmm(U, diag_S)\n",
        "        # V = torch.matmul(U, V)\n",
        "\n",
        "        # make diag_S sparse matrix\n",
        "        indices = torch.tensor((range(0, len(S)), range(0, len(S)))).to(device)\n",
        "        values = S\n",
        "        diag_S = torch.sparse.FloatTensor(indices, values, torch.Size((len(S), len(S))))\n",
        "        V = torch.spmm(diag_S, V)\n",
        "        V = torch.matmul(U, V)\n",
        "        return V\n",
        "\n",
        "    def prox_nuclear_truncated(self, data, alpha, k=50):\n",
        "        device = data.device\n",
        "        indices = torch.nonzero(data).t()\n",
        "        values = data[indices[0], indices[1]] # modify this based on dimensionality\n",
        "        data_sparse = sp.csr_matrix((values.cpu().numpy(), indices.cpu().numpy()))\n",
        "        U, S, V = sp.linalg.svds(data_sparse, k=k)\n",
        "        U, S, V = torch.FloatTensor(U).to(device), torch.FloatTensor(S).to(device), torch.FloatTensor(V).to(device)\n",
        "        self.nuclear_norm = S.sum()\n",
        "        diag_S = torch.diag(torch.clamp(S-alpha, min=0))\n",
        "        return torch.matmul(torch.matmul(U, diag_S), V)\n",
        "\n",
        "    def prox_nuclear_cuda(self, data, alpha):\n",
        "\n",
        "        device = data.device\n",
        "        U, S, V = torch.svd(data)\n",
        "        # self.nuclear_norm = S.sum()\n",
        "        # print(f\"rank = {len(S.nonzero())}\")\n",
        "        self.nuclear_norm = S.sum()\n",
        "        S = torch.clamp(S-alpha, min=0)\n",
        "        indices = torch.tensor([range(0, U.shape[0]),range(0, U.shape[0])]).to(device)\n",
        "        values = S\n",
        "        diag_S = torch.sparse.FloatTensor(indices, values, torch.Size(U.shape))\n",
        "        # diag_S = torch.diag(torch.clamp(S-alpha, min=0))\n",
        "        # print(f\"rank_after = {len(diag_S.nonzero())}\")\n",
        "        V = torch.spmm(diag_S, V.t_())\n",
        "        V = torch.matmul(U, V)\n",
        "        return V\n",
        "\n",
        "\n",
        "class SGD(Optimizer):\n",
        "\n",
        "\n",
        "    def __init__(self, params, lr=required, momentum=0, dampening=0,\n",
        "                 weight_decay=0, nesterov=False):\n",
        "        if lr is not required and lr < 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if momentum < 0.0:\n",
        "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "\n",
        "        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\n",
        "                        weight_decay=weight_decay, nesterov=nesterov)\n",
        "        if nesterov and (momentum <= 0 or dampening != 0):\n",
        "            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n",
        "        super(SGD, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(SGD, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('nesterov', False)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            weight_decay = group['weight_decay']\n",
        "            momentum = group['momentum']\n",
        "            dampening = group['dampening']\n",
        "            nesterov = group['nesterov']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                d_p = p.grad.data\n",
        "                if weight_decay != 0:\n",
        "                    d_p.add_(weight_decay, p.data)\n",
        "                if momentum != 0:\n",
        "                    param_state = self.state[p]\n",
        "                    if 'momentum_buffer' not in param_state:\n",
        "                        buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
        "                    else:\n",
        "                        buf = param_state['momentum_buffer']\n",
        "                        buf.mul_(momentum).add_(1 - dampening, d_p)\n",
        "                    if nesterov:\n",
        "                        d_p = d_p.add(momentum, buf)\n",
        "                    else:\n",
        "                        d_p = buf\n",
        "\n",
        "                p.data.add_(-group['lr'], d_p)\n",
        "\n",
        "        return loss\n",
        "\n",
        "prox_operators = ProxOperators()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lB7Qgv4nxYnJ"
      },
      "source": [
        "##Pro-GNN##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzq68ipGxb1G"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import time\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "# from deeprobust.graph.utils import accuracy\n",
        "# from deeprobust.graph.defense.pgd import PGD, prox_operators\n",
        "import warnings\n",
        "\n",
        "class ProGNN:\n",
        "    \"\"\" ProGNN (Properties Graph Neural Network). See more details in Graph Structure Learning for Robust Graph Neural Networks, KDD 2020, https://arxiv.org/abs/2005.10203.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model:\n",
        "        model: The backbone GNN model in ProGNN\n",
        "    args:\n",
        "        model configs\n",
        "    device: str\n",
        "        'cpu' or 'cuda'.\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    See details in https://github.com/ChandlerBang/Pro-GNN.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_f, model_c, args, device):\n",
        "        self.device = device\n",
        "        self.args = args\n",
        "        self.best_val_acc = 0\n",
        "        self.best_val_loss = 10\n",
        "        self.best_graph = None\n",
        "        self.weights_f = None\n",
        "        self.weights_c = None\n",
        "        self.estimator = None\n",
        "        self.model_f = model_f.to(device)\n",
        "        self.model_c = model_c.to(device)\n",
        "\n",
        "\n",
        "    def fit(self, features, adj, labels, idx_train, idx_val, opt,is_airport=False, title=\"\", **kwargs ): ########################added opt as an argument\n",
        "        \"\"\"Train Pro-GNN.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        features :\n",
        "            node features\n",
        "        adj :\n",
        "            the adjacency matrix. The format could be torch.tensor or scipy matrix\n",
        "        labels :\n",
        "            node labels\n",
        "        idx_train :\n",
        "            node training indices\n",
        "        idx_val :\n",
        "            node validation indices\n",
        "        \"\"\"\n",
        "        args = self.args\n",
        "\n",
        "        #self.optimizer = optim.Adam(self.model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
        "        self.optimizer_f = RiemannianSGD(self.model_f.optim_params(), lr=opt.lr_e) ####################### new optimizer for model_f\n",
        "        self.optimizer_c = torch.optim.Adam(self.model_c.parameters(), lr=opt.lr_c) ###################### new optimizer for model_c\n",
        "\n",
        "        estimator = EstimateAdj(adj, symmetric=args.symmetric, device=self.device).to(self.device)\n",
        "        self.estimator = estimator\n",
        "        self.optimizer_adj = optim.SGD(estimator.parameters(),\n",
        "                              momentum=0.9, lr=args.lr_adj)\n",
        "\n",
        "        self.optimizer_l1 = PGD(estimator.parameters(),\n",
        "                        proxs=[prox_operators.prox_l1],\n",
        "                        lr=args.lr_adj, alphas=[args.alpha])\n",
        "\n",
        "        # warnings.warn(\"If you find the nuclear proximal operator runs too slow on Pubmed, you can  uncomment line 67-71 and use prox_nuclear_cuda to perform the proximal on gpu.\")\n",
        "        # if args.dataset == \"pubmed\":\n",
        "        #     self.optimizer_nuclear = PGD(estimator.parameters(),\n",
        "        #               proxs=[prox_operators.prox_nuclear_cuda],\n",
        "        #               lr=args.lr_adj, alphas=[args.beta])\n",
        "        # else:\n",
        "        warnings.warn(\"If you find the nuclear proximal operator runs too slow, you can modify line 77 to use prox_operators.prox_nuclear_cuda instead of prox_operators.prox_nuclear to perform the proximal on GPU. See details in https://github.com/ChandlerBang/Pro-GNN/issues/1\")\n",
        "        self.optimizer_nuclear = PGD(estimator.parameters(),\n",
        "                  proxs=[prox_operators.prox_nuclear_cuda],\n",
        "                  lr=args.lr_adj, alphas=[args.beta])\n",
        "\n",
        "        # Train model\n",
        "        t_total = time.time()\n",
        "        for epoch in range(args.epochs):\n",
        "            #self, features, labels, idx_test,adj, order\n",
        "          \"\"\"\n",
        "            if args.only_gcn:\n",
        "                self.train_gcn(epoch, features, estimator.estimated_adj,\n",
        "                        labels, idx_train, idx_val)\n",
        "            else:\n",
        "          \"\"\"\n",
        "          for i in range(int(args.outer_steps)):\n",
        "              self.train_adj(epoch, features, adj, labels,\n",
        "                      idx_train, idx_val,opt, is_airport)\n",
        "\n",
        "          for i in range(int(args.inner_steps)):\n",
        "              self.train_gcn(epoch, features, estimator.estimated_adj,\n",
        "                      labels, idx_train, idx_val,opt,is_airport)\n",
        "\n",
        "        # filename = \"cleaned_adj\"+title+\".npy\"\n",
        "        # np.save(filename, self.estimator.normalize().detach().cpu())\n",
        "        self.save_hyla_features(title)\n",
        "        print(\"Optimization Finished!\")\n",
        "        print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "        print(args)\n",
        "\n",
        "        # Testing\n",
        "        # #print(\"picking the best model according to validation performance\")\n",
        "        # self.model_f.load_state_dict(self.weights_f)\n",
        "        # self.model_c.load_state_dict(self.weights_c)\n",
        "\n",
        "    def train_gcn(self, epoch, features, adj, labels, idx_train, idx_val, opt,is_airport): #added opt argument!\n",
        "        self.hyla_features_list = []\n",
        "        args = self.args\n",
        "        estimator = self.estimator\n",
        "        adj = estimator.normalize()\n",
        "        t = time.time()\n",
        "        self.model_f.train()\n",
        "        self.model_c.train()\n",
        "        self.optimizer_f.zero_grad()\n",
        "        self.optimizer_c.zero_grad()\n",
        "        HyLa_features = self.model_f()\n",
        "        if (is_airport):\n",
        "          features = adj\n",
        "          new_features, nonzero_perc = sgc_precompute(adj, features, opt.order-1)\n",
        "          features_train = new_features[idx_train]\n",
        "        else:\n",
        "          new_features, nonzero_perc = sgc_precompute(adj, features, opt.order)\n",
        "          features_train = new_features[idx_train]\n",
        "\n",
        "        ###############till here################################################\n",
        "        HyLa_features =torch.mm(features_train.to(opt.device), HyLa_features)\n",
        "        predictions = self.model_c(HyLa_features)  #predict using model_c (whose input is the output of hyla)\n",
        "        del HyLa_features #delete intermediate hyla features to free up memory\n",
        "        loss_train = F.cross_entropy(predictions, labels[idx_train].to(opt.device))\n",
        "        # Backpropagate the gradients and perform a step of optimization for both models\n",
        "        loss_train.backward()\n",
        "        self.optimizer_f.step()\n",
        "        self.optimizer_c.step()\n",
        "        acc_train, f1_train = acc_f1(predictions, labels[idx_train].to(opt.device))\n",
        "\n",
        "        # Evaluate validation set performance separately,\n",
        "        # deactivates dropout during validation run.\n",
        "        self.model_f.eval()\n",
        "        self.model_c.eval()\n",
        "        # Obtain HyLa_features by applying model_f to the input features\n",
        "        HyLa_features = self.model_f()\n",
        "        self.hyla_features_list.append(HyLa_features.detach().cpu().numpy())\n",
        "\n",
        "        features_val= new_features[idx_val]\n",
        "        HyLa_features = torch.mm(features_val.to(HyLa_features.device), HyLa_features)\n",
        "        predictions = self.model_c(HyLa_features) # Make predictions using model_c on the transformed features\n",
        "        del HyLa_features\n",
        "        acc, f1 = acc_f1(predictions, labels[idx_val].to(opt.device)) # Calculate accuracy and F1 score using the acc_f1 function\n",
        "        loss_val = F.cross_entropy(predictions, labels[idx_val].to(opt.device))\n",
        "        acc_val,f1_val = acc_f1(predictions, labels[idx_val].to(opt.device))\n",
        "\n",
        "        if acc_val > self.best_val_acc:\n",
        "            self.best_val_acc = acc_val\n",
        "            self.best_graph = adj.detach()\n",
        "            self.weights_f = deepcopy(self.model_f.state_dict())\n",
        "            self.weights_c = deepcopy(self.model_c.state_dict())\n",
        "            if args.debug:\n",
        "                print('\\t=== saving current graph/gcn, best_val_acc: %s' % self.best_val_acc.item())\n",
        "\n",
        "        if loss_val < self.best_val_loss:\n",
        "            self.best_val_loss = loss_val\n",
        "            self.best_graph = adj.detach()\n",
        "            self.weights_f = deepcopy(self.model_f.state_dict())\n",
        "            self.weights_c = deepcopy(self.model_c.state_dict())\n",
        "            if args.debug:\n",
        "                print(f'\\t=== saving current graph/gcn, best_val_loss: %s' % self.best_val_loss.item())\n",
        "\n",
        "        #if args.debug:\n",
        "\n",
        "        print('Epoch: {:04d}'.format(epoch+1),\n",
        "              'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "              'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "              'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "              'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "              'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "    def save_hyla_features(self, title=\"\"):\n",
        "        # Save the list containing all HyLa features as a numpy array\n",
        "        hyla_features_array = np.concatenate(self.hyla_features_list, axis=0)\n",
        "        filename = \"hyla_features\" + title + \".npy\"\n",
        "        np.save(filename, hyla_features_array)\n",
        "\n",
        "    def train_adj(self, epoch, features, adj, labels, idx_train, idx_val, opt,is_airport): #added another argument (opt)\n",
        "        estimator = self.estimator\n",
        "        args = self.args\n",
        "        if args.debug:\n",
        "            print(\"\\n=== train_adj ===\")\n",
        "        t = time.time()\n",
        "        estimator.train()\n",
        "        self.optimizer_adj.zero_grad()\n",
        "\n",
        "        loss_l1 = torch.norm(estimator.estimated_adj, 1)\n",
        "        loss_fro = torch.norm(estimator.estimated_adj - adj, p='fro')\n",
        "        normalized_adj = estimator.normalize()\n",
        "\n",
        "        if args.lambda_:\n",
        "            loss_smooth_feat = self.feature_smoothing(estimator.estimated_adj, features)\n",
        "        else:\n",
        "            loss_smooth_feat = 0 * loss_l1\n",
        "\n",
        "        self.model_f.eval()\n",
        "        self.model_c.eval()\n",
        "        # Obtain HyLa_features by applying model_f to the input features\n",
        "        HyLa_features = self.model_f()\n",
        "        if (is_airport):\n",
        "          features = adj\n",
        "          new_features, nonzero_perc = sgc_precompute(adj, features, opt.order-1)\n",
        "          features_train = new_features[idx_train]\n",
        "        else:\n",
        "          new_features, nonzero_perc = sgc_precompute(adj, features, opt.order)\n",
        "          features_train = new_features[idx_train]\n",
        "\n",
        "        HyLa_features = torch.mm(new_features[idx_train].to(HyLa_features.device), HyLa_features)\n",
        "        output = self.model_c(HyLa_features) # Make predictions using model_c on the transformed features\n",
        "        del HyLa_features\n",
        "\n",
        "        loss_gcn = F.cross_entropy(output, labels[idx_train].to(opt.device))\n",
        "        acc_train, f1_train = acc_f1(output, labels[idx_train])\n",
        "\n",
        "        loss_symmetric = torch.norm(estimator.estimated_adj \\\n",
        "                        - estimator.estimated_adj.t(), p=\"fro\")\n",
        "\n",
        "        loss_diffiential =  loss_fro + args.gamma * loss_gcn + args.lambda_ * loss_smooth_feat + args.phi * loss_symmetric\n",
        "\n",
        "        loss_diffiential.backward()\n",
        "\n",
        "        self.optimizer_adj.step()\n",
        "        loss_nuclear =  0 * loss_fro\n",
        "        if args.beta != 0:\n",
        "            self.optimizer_nuclear.zero_grad()\n",
        "            self.optimizer_nuclear.step()\n",
        "            loss_nuclear = prox_operators.nuclear_norm\n",
        "\n",
        "        self.optimizer_l1.zero_grad()\n",
        "        self.optimizer_l1.step()\n",
        "\n",
        "        total_loss = loss_fro \\\n",
        "                    + args.gamma * loss_gcn \\\n",
        "                    + args.alpha * loss_l1 \\\n",
        "                    + args.beta * loss_nuclear \\\n",
        "                    + args.phi * loss_symmetric\n",
        "\n",
        "        estimator.estimated_adj.data.copy_(torch.clamp(\n",
        "                  estimator.estimated_adj.data, min=0, max=1))\n",
        "\n",
        "        # Evaluate validation set performance separately,\n",
        "        # deactivates dropout during validation run.\n",
        "        normalized_adj = estimator.normalize()\n",
        "        if (is_airport):\n",
        "          features = normalized_adj\n",
        "          new_features, nonzero_perc = sgc_precompute(normalized_adj, features, opt.order-1)\n",
        "          features_train = new_features[idx_train]\n",
        "        else:\n",
        "          new_features, nonzero_perc = sgc_precompute(normalized_adj, features, opt.order)\n",
        "          features_train = new_features[idx_train]\n",
        "        self.model_f.eval()\n",
        "        self.model_c.eval()\n",
        "        HyLa_features = self.model_f()\n",
        "        HyLa_features = torch.mm(new_features[idx_val].to(HyLa_features.device), HyLa_features)\n",
        "        output = self.model_c(HyLa_features)\n",
        "\n",
        "        loss_val = F.cross_entropy(output, labels[idx_val].to(opt.device))\n",
        "        acc_val,f1_val = acc_f1(output, labels[idx_val].to(opt.device))\n",
        "        if (args.debug):\n",
        "          print('Epoch: {:04d}'.format(epoch+1),\n",
        "                'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "                'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "                'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "                'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "        if acc_val > self.best_val_acc:\n",
        "            self.best_val_acc = acc_val\n",
        "            self.best_graph = normalized_adj.detach()\n",
        "            self.weights_f = deepcopy(self.model_f.state_dict())\n",
        "            self.weights_c = deepcopy(self.model_c.state_dict())\n",
        "            if args.debug:\n",
        "                print(f'\\t=== saving current graph/gcn, best_val_acc: %s' % self.best_val_acc.item())\n",
        "\n",
        "        if loss_val < self.best_val_loss:\n",
        "            self.best_val_loss = loss_val\n",
        "            self.best_graph = normalized_adj.detach()\n",
        "            self.weights_f = deepcopy(self.model_f.state_dict())\n",
        "            self.weights_c = deepcopy(self.model_c.state_dict())\n",
        "            if args.debug:\n",
        "                print(f'\\t=== saving current graph/gcn, best_val_loss: %s' % self.best_val_loss.item())\n",
        "\n",
        "        if args.debug:\n",
        "            if epoch % 1 == 0:\n",
        "                print('Epoch: {:04d}'.format(epoch+1),\n",
        "                      'loss_fro: {:.4f}'.format(loss_fro.item()),\n",
        "                      'loss_gcn: {:.4f}'.format(loss_gcn.item()),\n",
        "                      'loss_feat: {:.4f}'.format(loss_smooth_feat.item()),\n",
        "                      'loss_symmetric: {:.4f}'.format(loss_symmetric.item()),\n",
        "                      'delta_l1_norm: {:.4f}'.format(torch.norm(estimator.estimated_adj-adj, 1).item()),\n",
        "                      'loss_l1: {:.4f}'.format(loss_l1.item()),\n",
        "                      'loss_total: {:.4f}'.format(total_loss.item()),\n",
        "                      'loss_nuclear: {:.4f}'.format(loss_nuclear.item()))\n",
        "\n",
        "\n",
        "    def test(self, features, labels, idx_test,adj, order, is_airport=False, title=\"\"):\n",
        "        \"\"\"Evaluate the performance of ProGNN on test set\n",
        "        \"\"\"\n",
        "        print(\"\\t=== testing ===\")\n",
        "        with torch.no_grad():\n",
        "          # Set both model_f and model_c to evaluation mode\n",
        "          self.model_f.eval()\n",
        "          self.model_c.eval()\n",
        "          # Obtain HyLa_features by applying model_f to the input features\n",
        "          HyLa_features = self.model_f()\n",
        "          normalized_adj = self.estimator.normalize()\n",
        "          if (is_airport):\n",
        "            features = normalized_adj\n",
        "            new_features, nonzero_perc = sgc_precompute(normalized_adj, features, order-1)\n",
        "          else:\n",
        "            new_features, nonzero_perc = sgc_precompute(normalized_adj, features, order)\n",
        "          HyLa_features = torch.mm(new_features[idx_test].to(HyLa_features.device), HyLa_features)\n",
        "          predictions = self.model_c(HyLa_features) # Make predictions using model_c on the transformed features\n",
        "          numpy_array = HyLa_features.cpu().numpy()\n",
        "          # filename = \"prohyla_features\"+title+\".npy\"\n",
        "          # np.save(filename, numpy_array)\n",
        "          del HyLa_features\n",
        "          acc, f1 = acc_f1(predictions, labels[idx_test]) # Calculate accuracy and F1 score using the acc_f1 function\n",
        "        return acc,f1\n",
        "\n",
        "    def feature_smoothing(self, adj, X):\n",
        "        adj = (adj.t() + adj)/2\n",
        "        rowsum = adj.sum(1)\n",
        "        r_inv = rowsum.flatten()\n",
        "        D = torch.diag(r_inv)\n",
        "        L = D - adj\n",
        "\n",
        "        L = L.to(torch.float64)\n",
        "        D = D.to(torch.float64)\n",
        "        r_inv = r_inv.to(torch.float64)\n",
        "\n",
        "\n",
        "        r_inv = r_inv  + 1e-3\n",
        "        r_inv = r_inv.pow(-1/2).flatten()\n",
        "        r_inv[torch.isinf(r_inv)] = 0.\n",
        "        r_mat_inv = torch.diag(r_inv)\n",
        "        # L = r_mat_inv @ L\n",
        "\n",
        "        L = r_mat_inv @ L @ r_mat_inv\n",
        "        XLXT = torch.matmul(torch.matmul(X.t(), L), X)\n",
        "        loss_smooth_feat = torch.trace(XLXT)\n",
        "        return loss_smooth_feat\n",
        "\n",
        "\n",
        "class EstimateAdj(nn.Module):\n",
        "    \"\"\"Provide a pytorch parameter matrix for estimated\n",
        "    adjacency matrix and corresponding operations.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, adj, symmetric=False, device='cpu'):\n",
        "        super(EstimateAdj, self).__init__()\n",
        "        n = len(adj)\n",
        "        self.estimated_adj = nn.Parameter(torch.FloatTensor(n, n))\n",
        "        self._init_estimation(adj)\n",
        "        self.symmetric = symmetric\n",
        "        self.device = device\n",
        "\n",
        "    def _init_estimation(self, adj):\n",
        "        with torch.no_grad():\n",
        "            n = len(adj)\n",
        "            self.estimated_adj.data.copy_(adj)\n",
        "\n",
        "    def forward(self):\n",
        "        return self.estimated_adj\n",
        "\n",
        "    def normalize(self):\n",
        "\n",
        "        if self.symmetric:\n",
        "            adj = (self.estimated_adj + self.estimated_adj.t())/2\n",
        "        else:\n",
        "            adj = self.estimated_adj\n",
        "\n",
        "        normalized_adj = self._normalize(adj + torch.eye(adj.shape[0]).to(self.device))\n",
        "        return normalized_adj\n",
        "\n",
        "    def _normalize(self, mx):\n",
        "        rowsum = mx.sum(1)\n",
        "        r_inv = rowsum.pow(-1/2).flatten()\n",
        "        r_inv[torch.isinf(r_inv)] = 0.\n",
        "        r_mat_inv = torch.diag(r_inv)\n",
        "        mx = r_mat_inv @ mx\n",
        "        mx = mx @ r_mat_inv\n",
        "        return mx\n",
        "\n",
        "##########################see loss details, merge!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVACPyk5QFNW"
      },
      "outputs": [],
      "source": [
        "        # ###############pasting here###########################################\n",
        "        # if (is_tsne):\n",
        "        #   tsne_model = TSNE(n_components=2, random_state=42)\n",
        "        #   # Fit and transform your data using t-SNE\n",
        "        #   X_tsne = tsne_model.fit_transform(new_features)\n",
        "        #   colors = plt.cm.tab10(labels)\n",
        "        #   # Plot the results with colors based on labels\n",
        "        #   plt.title('Raw Input Features visualized using t-SNE')\n",
        "        #   plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=colors)\n",
        "        #   plt.savefig(f'/content/raw_features.jpg', bbox_inches='tight', dpi=600)\n",
        "        #   plt.show()\n",
        "\n",
        "        # #print(HyLa_features.shape)\n",
        "        # if (epoch==opt.epochs-1 and is_tsne):\n",
        "        #   #related to t_sne\n",
        "        #   HyLa_features2 = self.model_f()\n",
        "        #   HyLa_features2 = torch.mm(new_features.to(opt.device), HyLa_features2)\n",
        "        #   X_tsne = tsne_model.fit_transform(HyLa_features2.cpu().detach().numpy())\n",
        "        #   plt.title('HyLa output Features visualized using t-SNE')\n",
        "        #   plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=colors)\n",
        "        #   plt.savefig(f'/content/hyla_features.jpg', bbox_inches='tight', dpi=600)\n",
        "        #   plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0ZqESgsXreF"
      },
      "source": [
        "### Models ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0rIqnCdXt1B"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Module\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "class HyLa(nn.Module):\n",
        "    def __init__(self, manifold, dim, size, HyLa_fdim, scale=0.1, sparse=False, **kwargs):\n",
        "        super(HyLa, self).__init__()\n",
        "        self.manifold = manifold\n",
        "        self.lt = manifold.allocate_lt(size, dim, sparse)\n",
        "        self.manifold.init_weights(self.lt)\n",
        "        self.dim = dim\n",
        "        self.Lambdas = scale * torch.randn(HyLa_fdim)\n",
        "        self.boundary = sample_boundary(HyLa_fdim, self.dim, cls='RandomUniform')\n",
        "        self.bias = 2 * np.pi * torch.rand(HyLa_fdim)\n",
        "\n",
        "    def forward(self):\n",
        "        with torch.no_grad():\n",
        "            e_all = self.manifold.normalize(self.lt.weight)\n",
        "        PsK = PoissonKernel(e_all, self.boundary.to(e_all.device))\n",
        "        angles = self.Lambdas.to(e_all.device)/2.0 * torch.log(PsK)\n",
        "        eigs = torch.cos(angles + self.bias.to(e_all.device)) * torch.sqrt(PsK)**(self.dim-1)\n",
        "        return eigs\n",
        "\n",
        "    def optim_params(self):\n",
        "        return [{\n",
        "            'params': self.lt.parameters(),\n",
        "            'rgrad': self.manifold.rgrad,\n",
        "            'expm': self.manifold.expm,\n",
        "            'logm': self.manifold.logm,\n",
        "            'ptransp': self.manifold.ptransp,\n",
        "        }]\n",
        "\n",
        "\n",
        "class RFF(nn.Module):\n",
        "    def __init__(self, manifold, dim, size, HyLa_fdim, scale=0.1, sparse=False, **kwargs):\n",
        "        super(RFF, self).__init__()\n",
        "        self.manifold = manifold\n",
        "        self.lt = manifold.allocate_lt(size, dim, sparse)\n",
        "        self.manifold.init_weights(self.lt)\n",
        "        self.norm = 1. / np.sqrt(dim)\n",
        "        self.Lambdas = nn.Parameter(torch.from_numpy(np.random.normal(loc=0, scale=scale, size=(dim, HyLa_fdim))), requires_grad=False)\n",
        "        self.bias = nn.Parameter(torch.from_numpy(np.random.uniform(0, 2 * np.pi, size=HyLa_fdim)),requires_grad=False)\n",
        "\n",
        "    def forward(self):\n",
        "        with torch.no_grad():\n",
        "            e_all = self.manifold.normalize(self.lt.weight)\n",
        "        features = self.norm * np.sqrt(2) * torch.cos(e_all @ self.Lambdas + self.bias)\n",
        "        return features\n",
        "\n",
        "    def optim_params(self):\n",
        "        return [{\n",
        "            'params': self.lt.parameters(),\n",
        "            'rgrad': self.manifold.rgrad,\n",
        "            'expm': self.manifold.expm,\n",
        "            'logm': self.manifold.logm,\n",
        "            'ptransp': self.manifold.ptransp,\n",
        "        }]\n",
        "\n",
        "class SGC(nn.Module):\n",
        "    \"\"\"\n",
        "    A Simple PyTorch Implementation of Logistic Regression.\n",
        "    Assuming the features have been preprocessed with k-step graph propagation.\n",
        "    \"\"\"\n",
        "    def __init__(self, nfeat, nclass):\n",
        "        super(SGC, self).__init__()\n",
        "\n",
        "        self.W = nn.Linear(nfeat, nclass)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.W(x)\n",
        "\n",
        "def build_model(opt, N):\n",
        "    if isinstance(opt, argparse.Namespace):\n",
        "        opt = vars(opt)\n",
        "    manifold = MANIFOLDS[opt['manifold']](K=None)\n",
        "    return MODELS[opt['model']](\n",
        "        manifold,\n",
        "        dim=opt['he_dim'],\n",
        "        size=N,\n",
        "        HyLa_fdim=opt['hyla_dim'],\n",
        "        scale=opt['lambda_scale'],\n",
        "        sparse=opt['sparse'],\n",
        "    )\n",
        "\n",
        "def get_model(model_opt, nfeat, nclass, adj=None, dropout=0.0):\n",
        "    if model_opt == \"SGC\":\n",
        "        model = SGC(nfeat=nfeat,\n",
        "                    nclass=nclass)\n",
        "    else:\n",
        "        raise NotImplementedError('model:{} is not implemented!'.format(model_opt))\n",
        "    return model\n",
        "\n",
        "MANIFOLDS = {\n",
        "    'lorentz': LorentzManifold,\n",
        "    'poincare': PoincareManifold,\n",
        "    'euclidean': EuclideanManifold,\n",
        "}\n",
        "\n",
        "MODELS = {\n",
        "    'hyla': HyLa,\n",
        "    'rff': RFF,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0-N89mNoUDU"
      },
      "source": [
        "##Mounting google drive for ease of accesing datasets##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7VbT0HaoPjy",
        "outputId": "7db221cf-0f82-4e85-8b37-5e6600411759"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8Yw5PHasgrw"
      },
      "source": [
        "**Training loop**\n",
        "\n",
        "\n",
        "\n",
        "*   **Generate_ckpt:** This function is designed to create or load a checkpoint for a machine learning model. It initializes or loads the checkpoint, updates the model's state, and adjusts the starting epoch based on the checkpoint information. The specific behavior of the checkpointing process depends on the implementation details of the LocalCheckpoint class, which is not provided in the code snippet.\n",
        "*   **Test_regression(model_f, model_c, features, test_labels, test_index=None, metric='acc'):** Perform predictions on features (after converting raw features to hyla output) then uses model_c to to perform classification and returns accuracy/F1 score.\n",
        "* **train(model_f,model_c,optimizer_f,optimizer_c,data,opt,log,progress=False,ckps=None,is_tsne=False):** model_f is the hyla model and model_c is the classification model (here SGC), data is a dictionary that contains node features, adjacency matrix, training, val and test sets. is_tsne by default is false because it is computationally expensive, when set true, gives the tsne visualizations of hyla features and raw input features\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ervYjw0tshV3"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import os\n",
        "import sys\n",
        "import inspect\n",
        "import numpy as np\n",
        "import torch\n",
        "import logging\n",
        "import argparse\n",
        "import json\n",
        "import torch.nn.functional as F\n",
        "import timeit\n",
        "import gc\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "def generate_ckpt(opt, model, path):\n",
        "    checkpoint = LocalCheckpoint(\n",
        "            path,\n",
        "            include_in_all={'conf' : vars(opt)},\n",
        "            start_fresh=opt.fresh\n",
        "        )\n",
        "    # get state from checkpoint\n",
        "    state = checkpoint.initialize({'epoch': 0, 'model': model.state_dict()})\n",
        "    model.load_state_dict(state['model'])\n",
        "    opt.epoch_start = state['epoch']\n",
        "    return checkpoint\n",
        "\n",
        "def test_regression(model_f, model_c, features, test_labels, test_index=None, metric='acc', title=\"\"):\n",
        "    with torch.no_grad():\n",
        "        # Set both model_f and model_c to evaluation mode\n",
        "        model_f.eval()\n",
        "        model_c.eval()\n",
        "        # Obtain HyLa_features by applying model_f to the input features\n",
        "        HyLa_features = model_f()\n",
        "        HyLa_features = torch.mm(features.to(HyLa_features.device), HyLa_features)\n",
        "        predictions = model_c(HyLa_features) # Make predictions using model_c on the transformed features\n",
        "        numpy_array = HyLa_features.cpu().numpy()\n",
        "        s = \"hyla_features\"+title+\".npy\"\n",
        "        np.save(s, numpy_array)\n",
        "        del HyLa_features\n",
        "        acc, f1 = acc_f1(predictions, test_labels) # Calculate accuracy and F1 score using the acc_f1 function\n",
        "    return acc,f1\n",
        "\n",
        "def train(model_f,\n",
        "          model_c,\n",
        "          optimizer_f,\n",
        "          optimizer_c,\n",
        "          data,\n",
        "          opt,\n",
        "          log,\n",
        "          progress=False,\n",
        "          ckps=None,\n",
        "          is_tsne=True,\n",
        "):\n",
        "\n",
        "    model_f.train()\n",
        "    model_c.train()\n",
        "    val_acc_best = 0.0\n",
        "    train_acc_best = 0.0\n",
        "    aux_var = False\n",
        "    epochs = []\n",
        "    training_curve = []\n",
        "    val_curve = []\n",
        "    # Create a t-SNE model\n",
        "    if (is_tsne):\n",
        "      tsne_model = TSNE(n_components=2, random_state=42)\n",
        "      # Fit and transform your data using t-SNE\n",
        "      X_tsne = tsne_model.fit_transform(data['features'])\n",
        "      colors = plt.cm.tab10(data['labels'])\n",
        "      # Plot the results with colors based on labels\n",
        "      plt.title('Raw Input Features visualized using t-SNE')\n",
        "      plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=colors)\n",
        "      plt.savefig(f'/content/raw_features.jpg', bbox_inches='tight', dpi=600)\n",
        "      plt.show()\n",
        "    for epoch in range(opt.epoch_start, opt.epochs):\n",
        "        t_start = timeit.default_timer()\n",
        "        # Zero out gradients for both optimizer_f and optimizer_c\n",
        "        optimizer_f.zero_grad()\n",
        "        optimizer_c.zero_grad()\n",
        "        HyLa_features = model_f()\n",
        "        HyLa_features = torch.mm(data['features_train'].to(opt.device), HyLa_features)\n",
        "        #print(HyLa_features.shape)\n",
        "        if (epoch==opt.epochs-1 and is_tsne):\n",
        "          #related to t_sne\n",
        "          HyLa_features2 = model_f()\n",
        "          HyLa_features2 = torch.mm(data['features'].to(opt.device), HyLa_features2)\n",
        "          X_tsne = tsne_model.fit_transform(HyLa_features2.cpu().detach().numpy())\n",
        "          plt.title('HyLa output Features visualized using t-SNE')\n",
        "          plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=colors)\n",
        "          plt.savefig(f'/content/hyla_features.jpg', bbox_inches='tight', dpi=600)\n",
        "          plt.show()\n",
        "          HyLa_features = model_f()\n",
        "          HyLa_features = torch.mm(data['features_train'].to(opt.device), HyLa_features)\n",
        "        predictions = model_c(HyLa_features)  #predict using model_c (whose input is the output of hyla)\n",
        "        del HyLa_features #delete intermediate hyla features to free up memory\n",
        "        loss = F.cross_entropy(predictions, data['labels'][data['idx_train']].to(opt.device))\n",
        "        # Backpropagate the gradients and perform a step of optimization for both models\n",
        "        loss.backward()\n",
        "        optimizer_f.step()\n",
        "        optimizer_c.step()\n",
        "        #update the metrics!\n",
        "        train_acc,train_f1 = test_regression(\n",
        "            model_f, model_c, data['features_train'], data['labels'][data['idx_train']].to(opt.device), metric = opt.metric)\n",
        "        val_acc,val_f1 = test_regression(model_f, model_c, data['features'][data['idx_val']],\n",
        "                                  data['labels'][data['idx_val']].to(opt.device), metric = opt.metric)\n",
        "        epochs.append(epoch)\n",
        "        training_curve.append(train_acc*100.0)\n",
        "        val_curve.append(val_acc*100.0)\n",
        "        if val_acc>val_acc_best:\n",
        "            val_acc_best = val_acc\n",
        "            if ckps is not None:\n",
        "                ckps[0].save({\n",
        "                'model': model_f.state_dict(),\n",
        "                'epoch': epoch,\n",
        "                'val_acc_best': val_acc_best,\n",
        "                })\n",
        "                ckps[1].save({\n",
        "                'model': model_c.state_dict(),\n",
        "                'epoch': epoch,\n",
        "                'val_acc_best': val_acc_best,\n",
        "                })\n",
        "        if train_acc>train_acc_best:\n",
        "            train_acc_best = train_acc\n",
        "        if progress:\n",
        "            log.info(\n",
        "                'running stats: {'\n",
        "                f'\"epoch\": {epoch}, '\n",
        "                f'\"elapsed\": {timeit.default_timer()-t_start:.2f}, '\n",
        "                f'\"train_acc\": {train_acc*100.0:.2f}%, '\n",
        "                f'\"val_acc\": {val_acc*100.0:.2f}%, '\n",
        "                f'\"loss_c\": {loss.cpu().item():.4f}, '\n",
        "                '}'\n",
        "            )\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(epochs, training_curve, label='Training Accuracy', marker='o')\n",
        "    plt.plot(epochs, val_curve, label='Validation Accuracy', marker='o')\n",
        "\n",
        "    # Adding labels and title\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training Curve')\n",
        "    plt.legend()  # Display legend\n",
        "\n",
        "    # Display the plot\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "    print(model_f)\n",
        "    print(model_c)\n",
        "    \"\"\"\n",
        "    return train_acc, train_acc_best, val_acc, val_acc_best"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-33Wf-qUR4O_"
      },
      "source": [
        "##Main HyLa Function##\n",
        "  * Add all the hyperparameters using new_args\n",
        "\n",
        "  * want_tsne (2nd argument of main) (default False), when set true, shows tsne visulization of raw input features and hyla output features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqB_T4QCeemC"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "#!/usr/bin/env python3\n",
        "\n",
        "def are_tensors_equal(tensor1, tensor2):\n",
        "    # Convert tensors to NumPy arrays\n",
        "    array1 = np.array(tensor1)\n",
        "    array2 = np.array(tensor2)\n",
        "\n",
        "    # Check if the arrays are equal\n",
        "    are_equal = np.array_equal(array1, array2)\n",
        "\n",
        "    return are_equal\n",
        "\n",
        "def main(new_args, want_tsne=False, adj_arg = None,feat_arg = None, labels = None, idx_test = None,idx_train = None,idx_val = None, proGNN=False, prognn_args = None, title = \"\"):\n",
        "    parser = argparse.ArgumentParser(description='Train HyLa-SGC for node classification tasks')\n",
        "    parser.add_argument('-checkpoint', action='store_true', default=False)\n",
        "    parser.add_argument('-task', type=str, default='nc', help='learning task')\n",
        "    parser.add_argument('-dataset', type=str, required=False, default = 'cora',\n",
        "                        help='Dataset identifier [cora|disease_nc|pubmed|citeseer|reddit|airport]')\n",
        "    parser.add_argument('-he_dim', type=int, default=2,\n",
        "                        help='Hyperbolic Embedding dimension')\n",
        "    parser.add_argument('-hyla_dim', type=int, default=100,\n",
        "                        help='HyLa feature dimension')\n",
        "    parser.add_argument('-order', type=int, default=2,\n",
        "                        help='order of adjaceny matrix in SGC precomputation')\n",
        "    parser.add_argument('-manifold', type=str, default='poincare',\n",
        "                        choices=MANIFOLDS.keys(), help='model of hyperbolic space')\n",
        "    parser.add_argument('-model', type=str, default='hyla',\n",
        "                        choices=MODELS.keys(), help='feature model class, hyla|rff')\n",
        "    parser.add_argument('-lr_e', type=float, default=0.1,\n",
        "                        help='Learning rate for hyperbolic embedding')\n",
        "    parser.add_argument('-lr_c', type=float, default=0.1,\n",
        "                        help='Learning rate for the classifier SGC')\n",
        "    parser.add_argument('-epochs', type=int, default=100,\n",
        "                        help='Number of epochs')\n",
        "    parser.add_argument('-strategy', type=int, default=0,\n",
        "                        help='Epochs of burn in, some advanced definition')\n",
        "    parser.add_argument('-eval_each', type=int, default=1,\n",
        "                        help='Run evaluation every n-th epoch')\n",
        "    parser.add_argument('-fresh', action='store_true', default=False,\n",
        "                        help='Override checkpoint')\n",
        "    parser.add_argument('-debug', action='store_true', default=False,\n",
        "                        help='Print debuggin output')\n",
        "    parser.add_argument('-gpu', default=0, type=int,\n",
        "                        help='Which GPU to run on (-1 for no gpu)')\n",
        "    parser.add_argument('-seed', default=43, type=int, help='random seed')\n",
        "    parser.add_argument('-sparse', default=True, action='store_true',\n",
        "                        help='Use sparse gradients for embedding table')\n",
        "    parser.add_argument('-quiet', action='store_true', default=True)\n",
        "    parser.add_argument('-lre_type', choices=['scale', 'constant'], default='constant')\n",
        "    parser.add_argument('-optim_type', choices=['adam', 'sgd'], default='adam', help='optimizer used for the classification SGC model')\n",
        "    parser.add_argument('-metric', choices=['acc', 'f1'], default='acc', help='what metrics to report')\n",
        "    parser.add_argument('-lambda_scale', type=float, default=0.07, help='scale of lambdas when generating HyLa features')\n",
        "    parser.add_argument('-inductive', action='store_true', default=False, help='inductive training, used for reddit.')\n",
        "    parser.add_argument('-use_feats', action='store_true', default=False, help='whether embed in the feature level, otherwise node level')\n",
        "    parser.add_argument('-tuned', action='store_true', default=False, help='whether use tuned hyper-parameters')\n",
        "    opt = parser.parse_args(new_args)\n",
        "\n",
        "    if opt.tuned:\n",
        "        with open(f'/content/drive/MyDrive/hyper_parameters_{opt.he_dim}d.json',) as f:\n",
        "          hyper_parameters = json.load(f)[opt.dataset]\n",
        "        opt.he_dim = hyper_parameters['he_dim']\n",
        "        opt.hyla_dim = hyper_parameters['hyla_dim']\n",
        "        opt.order = hyper_parameters['order']\n",
        "        opt.lambda_scale = hyper_parameters['lambda_scale']\n",
        "        opt.lr_e = hyper_parameters['lr_e']\n",
        "        opt.lr_c = hyper_parameters['lr_c']\n",
        "        opt.epochs = hyper_parameters['epochs']\n",
        "    # Set the evaluation metric based on the dataset\n",
        "    opt.metric = 'f1' if opt.dataset == 'reddit' else 'acc'\n",
        "\n",
        "    # Set the starting epoch for training\n",
        "    opt.epoch_start = 0\n",
        "\n",
        "    # Set random seeds for reproducibility\n",
        "    torch.manual_seed(opt.seed)\n",
        "    np.random.seed(opt.seed)\n",
        "\n",
        "    # Set split_seed for consistent data splits\n",
        "    opt.split_seed = opt.seed\n",
        "\n",
        "    # Display progress information during training unless opt.quiet is True\n",
        "    opt.progress = not opt.quiet\n",
        "\n",
        "    # Set up debugging and logging\n",
        "    log_level = logging.DEBUG if opt.debug else logging.INFO\n",
        "    log = logging.getLogger('HyLa')\n",
        "    logging.basicConfig(level=log_level, format='%(message)s', stream=sys.stdout)\n",
        "\n",
        "    # Set default tensor type to DoubleTensor\n",
        "    torch.set_default_tensor_type('torch.DoubleTensor')\n",
        "\n",
        "    # Set device for computation (GPU if available, otherwise CPU)\n",
        "    opt.device = torch.device(f'cuda:{opt.gpu}' if opt.gpu >= 0 and torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Specify the path to the dataset\n",
        "    data_path = f'/content/drive/MyDrive/datasets/{opt.dataset}/'\n",
        "\n",
        "    # Load data based on the dataset\n",
        "    if opt.dataset in ['cora', 'disease_nc', 'pubmed', 'citeseer', 'airport']:\n",
        "        data = load_data(opt, data_path)\n",
        "    elif opt.dataset in ['reddit']:\n",
        "        data = load_reddit_data(data_path)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    if (adj_arg!=None):\n",
        "        if (not proGNN):\n",
        "          adj_n = aug_normalized_adjacency(adj_arg)\n",
        "          data['adj_train'] = sparse_mx_to_torch_sparse_tensor(adj_n)\n",
        "        else:\n",
        "          data['adj_train'] = sparse_mx_to_torch_sparse_tensor(adj_arg)\n",
        "        # Precompute features for inductive learning or non-explicit features\n",
        "    if (feat_arg is not None):\n",
        "        data['features'] = feat_arg\n",
        "    if (labels is not None):\n",
        "        data['labels'] = labels\n",
        "    if (idx_train is not None):\n",
        "        data['idx_train'] = idx_train\n",
        "    if (idx_test is not None):\n",
        "        data['idx_test'] = idx_test\n",
        "    if (idx_val is not None):\n",
        "        data['idx_val'] = idx_val\n",
        "    # Set up dataset parameters and settings\n",
        "    if opt.use_feats or opt.inductive:\n",
        "        # Feature dimension when using hyperbolic Laplacian features at the feature level\n",
        "        feature_dim = data['features'].size(1)\n",
        "    else:\n",
        "        # Feature dimension when using hyperbolic Laplacian features at the node level\n",
        "        feature_dim = data['adj_train'].size(1)\n",
        "\n",
        "    # Display information about the data\n",
        "    if opt.progress:\n",
        "        log.info(f'Training set size: {len(data[\"idx_train\"])}, Validation set size: {len(data[\"idx_val\"])}, Test set size: {len(data[\"idx_test\"])}')\n",
        "        log.info(f'Size of original feature matrix: {data[\"features\"].size()}, Number of classes: {data[\"labels\"].max().item()+1}')\n",
        "        log.info('Precomputing features')\n",
        "    if opt.inductive:\n",
        "        features = data['features']\n",
        "        data['features'], _ = sgc_precompute(data['adj_all'], features, opt.order)\n",
        "        data['features_train'], nonzero_perc = sgc_precompute(data['adj_train'], features[data['idx_train']], opt.order)\n",
        "    else:\n",
        "        if not opt.use_feats:\n",
        "            if not proGNN:\n",
        "              features = data['adj_train'].to_dense()\n",
        "              data['features'], nonzero_perc = sgc_precompute(data['adj_train'], features, opt.order-1)\n",
        "              data['features_train'] = data['features'][data['idx_train']]\n",
        "            else:\n",
        "              features = data['adj_train'].to_dense()\n",
        "              data['features'], nonzero_perc = sgc_precompute(data['adj_train'], features, opt.order-1)\n",
        "              data['features_train'] = data['features'][data['idx_train']]\n",
        "        else:\n",
        "            features = data['features'].to_dense()\n",
        "            if not proGNN:\n",
        "              data['features'], nonzero_perc = sgc_precompute(data['adj_train'], features, opt.order)\n",
        "              data['features_train'] = data['features'][data['idx_train']]\n",
        "            else:\n",
        "              data['features_train'] = features[data['idx_train']]\n",
        "\n",
        "    # Display information about the percentage of non-zero elements during adjacency matrix precomputations\n",
        "    if opt.progress:\n",
        "        log.info(f'Nonzero percentage during adjacency matrix precomputations: {nonzero_perc}%')\n",
        "\n",
        "    # Build feature model and set up optimizer\n",
        "    model_f = build_model(opt, feature_dim).to(opt.device)\n",
        "\n",
        "    # Scale learning rate if lre_type is 'scale'\n",
        "    if opt.lre_type == 'scale':\n",
        "        opt.lr_e = opt.lr_e * len(data['idx_train'])\n",
        "\n",
        "    # Set up optimizer for feature model based on the specified manifold\n",
        "    if opt.manifold == 'euclidean':\n",
        "        optimizer_f = torch.optim.SGD(model_f.parameters(), lr=opt.lr_e)\n",
        "    elif opt.manifold == 'poincare':\n",
        "        optimizer_f = RiemannianSGD(model_f.optim_params(), lr=opt.lr_e)\n",
        "\n",
        "    # Build classification model and set up optimizer\n",
        "    model_c = get_model(\"SGC\", opt.hyla_dim, data['labels'].max().item()+1).to(opt.device)\n",
        "    if opt.optim_type == 'sgd':\n",
        "        optimizer_c = torch.optim.SGD(model_c.parameters(), lr=opt.lr_c)\n",
        "    elif opt.optim_type == 'adam':\n",
        "        optimizer_c = torch.optim.Adam(model_c.parameters(), lr=opt.lr_c)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # Set up checkpoints if enabled\n",
        "    ckps = None\n",
        "    if opt.checkpoint:\n",
        "        ckp_fm = generate_ckpt(opt, model_f, f'/content/drive/MyDrive/HyLa/HyLa-master/nc/datasets/{opt.dataset}/fm.pt')\n",
        "        ckp_cm = generate_ckpt(opt, model_c, f'/content/drive/MyDrive/HyLa/HyLa-master/nc/datasets/{opt.dataset}/cm.pt')\n",
        "        ckps = (ckp_fm, ckp_cm)\n",
        "\n",
        "    # Start training and record accuracies\n",
        "    t_start_all = timeit.default_timer()\n",
        "    if not proGNN:\n",
        "      train_acc, train_acc_best, val_acc, val_acc_best = train(\n",
        "          model_f, model_c, optimizer_f, optimizer_c,\n",
        "          data, opt, log, progress=opt.progress, ckps=ckps, is_tsne=want_tsne)\n",
        "    else:\n",
        "      prognn = ProGNN(model_f,model_c, prognn_args, opt.device)\n",
        "      # pth = f\"/content/drive/MyDrive/plots material/airport/airport_rnd/\"\n",
        "\n",
        "      # # adj = trainer.best_graph.cpu().detach()\n",
        "      # # torch.save(adj, pth + f\"clean_adj_{opt.ptb_lvl}.pt\")\n",
        "\n",
        "      # Hyla_model = prognn.model_f\n",
        "      # Z_emb = Hyla_model.lt\n",
        "      # torch.save(Z_emb, pth + f\"poincare_emb_max_rnd_epoch100.pt\")\n",
        "\n",
        "      # Hyla_model.eval()\n",
        "      # HyLa_features = Hyla_model()\n",
        "      # if opt.use_feats:\n",
        "      #     HyLa_features = torch.mm(data['features'].to(opt.device), HyLa_features)\n",
        "      # torch.save(HyLa_features, pth + f\"hyla_features_max_rnd_epoch100.pt\")\n",
        "\n",
        "      # print(\"here\")\n",
        "      if not opt.use_feats:\n",
        "\n",
        "        features = data['features']\n",
        "        prognn.fit(features.to(opt.device), data['adj_train'].to_dense().to(opt.device), data['labels'], data['idx_train'], data['idx_val'],opt,True,title)\n",
        "        # print(\"here\")\n",
        "\n",
        "      else:\n",
        "        features = data['features'].to_dense()\n",
        "        prognn.fit(features.to(opt.device), data['adj_train'].to_dense().to(opt.device), data['labels'], data['idx_train'], data['idx_val'],opt,title)\n",
        "\n",
        "        # hyla_model = prognn.model_c\n",
        "        ## poincare embedding\n",
        "\n",
        "\n",
        "      return prognn.test(features.to(opt.device), data['labels'], data['idx_test'], data['adj_train'].to_dense().to(opt.device), opt.order, not opt.use_feats,title)\n",
        "\n",
        "    # Display total elapsed time\n",
        "    if opt.progress:\n",
        "        log.info(f'TOTAL ELAPSED: {timeit.default_timer()-t_start_all:.2f}')\n",
        "    print(\"Total time taken: \", timeit.default_timer()-t_start_all)\n",
        "\n",
        "    # Load the best model from the checkpoints if applicable\n",
        "    if opt.checkpoint and ckps is not None:\n",
        "        state_fm = ckps[0].load()\n",
        "        state_cm = ckps[1].load()\n",
        "        model_f.load_state_dict(state_fm['model'])\n",
        "        model_c.load_state_dict(state_cm['model'])\n",
        "        if opt.progress:\n",
        "            log.info(f'Early stopping, loading from epoch: {state_fm[\"epoch\"]} with val_acc_best: {state_fm[\"val_acc_best\"]}')\n",
        "\n",
        "    # Test the model on the test set\n",
        "    if not proGNN:\n",
        "      test_acc,test_f1 = test_regression(model_f, model_c, data['features'][data['idx_test']], data['labels'][data['idx_test']].to(opt.device),metric='acc', title = title)\n",
        "      #test_f1 = test_regression(model_f, model_c, data['features'][data['idx_test']], data['labels'][data['idx_test']].to(opt.device),metric='f1')\n",
        "\n",
        "      # Display training and test accuracies\n",
        "      print(\"Training accuracy: \", train_acc * 100.0)\n",
        "      print(\"Best training accuracy: \", train_acc_best * 100.0)\n",
        "      print(\"Validation accuracy: \", val_acc * 100.0)\n",
        "      print(\"Best validation accuracy: \", val_acc_best * 100.0)\n",
        "      print(\"Test accuracy: \", test_acc * 100.0)\n",
        "      \"\"\"\n",
        "      epochs = range(1, len(train_acc) + 1)\n",
        "      plt.scatter(epochs, (train_acc * 100.0), label='Training Loss', marker='o')\n",
        "      plt.scatter(epochs, (val_acc * 100.0), label='Validation Loss', marker='o')\n",
        "      plt.xlabel('Epochs')\n",
        "      plt.ylabel('Loss')\n",
        "      plt.title('Training and Validation Loss')\n",
        "      plt.legend()\n",
        "      plt.show()\n",
        "      \"\"\"\n",
        "\n",
        "      # Log the results\n",
        "      log.info(\n",
        "          f'\"|| Last train_acc\": {train_acc*100.0:.2f}%, '\n",
        "          f'\"|| Best train_acc\": {train_acc_best*100.0:.2f}%, '\n",
        "          f'\"|| Last val_acc\": {val_acc*100.0:.2f}%, '\n",
        "          f'\"|| Best val_acc\": {val_acc_best*100.0:.2f}%, '\n",
        "          f'\"|| Test_acc\": {test_acc*100.0:.2f}%.'\n",
        "      )\n",
        "\n",
        "      return [test_acc*100.0,test_f1*100.0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SG5j9vMpOZKJ"
      },
      "source": [
        "## Airport Random Attack ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19-_epS4NsFh",
        "outputId": "8b6200f4-88f1-4e75-8fcd-210831320c7a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-11-6d7d438a4fee>:62: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:210.)\n",
            "  coo_indices = torch.tensor(feat_matrix.nonzero())\n",
            "<ipython-input-11-6d7d438a4fee>:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  arr_temp2 = main(new_args, adj_arg=sparse_adj_matrix,feat_arg=torch.sparse_coo_tensor(coo_indices, coo_values, size=size, dtype=torch.double),labels=torch.tensor(labels), idx_train=idx_train, idx_val=idx_val, idx_test=idx_test,proGNN=True, prognn_args=args)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2756\n",
            "144\n",
            "288\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-9a62b986550e>:81: UserWarning: If you find the nuclear proximal operator runs too slow, you can modify line 77 to use prox_operators.prox_nuclear_cuda instead of prox_operators.prox_nuclear to perform the proximal on GPU. See details in https://github.com/ChandlerBang/Pro-GNN/issues/1\n",
            "  warnings.warn(\"If you find the nuclear proximal operator runs too slow, you can modify line 77 to use prox_operators.prox_nuclear_cuda instead of prox_operators.prox_nuclear to perform the proximal on GPU. See details in https://github.com/ChandlerBang/Pro-GNN/issues/1\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0001 loss_train: 1.6350 acc_train: 0.1082 loss_val: 33.8945 acc_val: 0.4943 time: 16.7506s\n",
            "Epoch: 0001 loss_train: 27.2738 acc_train: 0.4400 loss_val: 48.1699 acc_val: 0.4943 time: 15.9232s\n",
            "Epoch: 0002 loss_train: 41.5547 acc_train: 0.4400 loss_val: 58.2063 acc_val: 0.2405 time: 16.3490s\n",
            "Epoch: 0002 loss_train: 45.7920 acc_train: 0.3505 loss_val: 39.4659 acc_val: 0.2405 time: 16.3310s\n",
            "Epoch: 0003 loss_train: 28.8695 acc_train: 0.3505 loss_val: 75.0300 acc_val: 0.1050 time: 16.0497s\n",
            "Epoch: 0003 loss_train: 69.8260 acc_train: 0.1014 loss_val: 52.1980 acc_val: 0.4943 time: 16.2668s\n",
            "Epoch: 0004 loss_train: 50.7360 acc_train: 0.4400 loss_val: 54.7884 acc_val: 0.4943 time: 16.1106s\n",
            "Epoch: 0004 loss_train: 54.8447 acc_train: 0.4400 loss_val: 46.1329 acc_val: 0.4943 time: 16.8006s\n",
            "Epoch: 0005 loss_train: 46.0287 acc_train: 0.4400 loss_val: 29.2347 acc_val: 0.4943 time: 16.0667s\n",
            "Epoch: 0005 loss_train: 27.6982 acc_train: 0.4400 loss_val: 14.0899 acc_val: 0.2405 time: 16.6890s\n",
            "Epoch: 0006 loss_train: 10.3033 acc_train: 0.3505 loss_val: 19.9524 acc_val: 0.2405 time: 16.1107s\n",
            "Epoch: 0006 loss_train: 16.5434 acc_train: 0.3505 loss_val: 17.0132 acc_val: 0.1603 time: 16.7598s\n",
            "Epoch: 0007 loss_train: 16.3324 acc_train: 0.1082 loss_val: 17.3486 acc_val: 0.1050 time: 16.0870s\n",
            "Epoch: 0007 loss_train: 16.3685 acc_train: 0.1014 loss_val: 9.5222 acc_val: 0.2405 time: 17.6387s\n",
            "Epoch: 0008 loss_train: 8.0108 acc_train: 0.3505 loss_val: 9.0932 acc_val: 0.4943 time: 16.4925s\n",
            "Epoch: 0008 loss_train: 8.9853 acc_train: 0.4400 loss_val: 12.8108 acc_val: 0.4943 time: 17.4529s\n",
            "Epoch: 0009 loss_train: 12.9828 acc_train: 0.4400 loss_val: 9.4458 acc_val: 0.4943 time: 17.0701s\n",
            "Epoch: 0009 loss_train: 8.8645 acc_train: 0.4400 loss_val: 15.5974 acc_val: 0.2405 time: 22.9592s\n",
            "Epoch: 0010 loss_train: 13.5741 acc_train: 0.3505 loss_val: 15.5183 acc_val: 0.2424 time: 16.8669s\n",
            "Epoch: 0010 loss_train: 13.8933 acc_train: 0.4070 loss_val: 19.0409 acc_val: 0.1603 time: 16.6593s\n",
            "Epoch: 0011 loss_train: 19.4605 acc_train: 0.1082 loss_val: 6.7284 acc_val: 0.5115 time: 16.8748s\n",
            "Epoch: 0011 loss_train: 6.6693 acc_train: 0.5193 loss_val: 8.8070 acc_val: 0.4943 time: 17.2560s\n",
            "Epoch: 0012 loss_train: 7.4270 acc_train: 0.4400 loss_val: 12.4149 acc_val: 0.2405 time: 16.9848s\n",
            "Epoch: 0012 loss_train: 9.4084 acc_train: 0.3505 loss_val: 9.6746 acc_val: 0.2405 time: 16.8631s\n",
            "Epoch: 0013 loss_train: 6.5719 acc_train: 0.3505 loss_val: 11.5499 acc_val: 0.4943 time: 16.6962s\n",
            "Epoch: 0013 loss_train: 9.3987 acc_train: 0.4400 loss_val: 10.5226 acc_val: 0.5191 time: 16.4391s\n",
            "Epoch: 0014 loss_train: 8.8851 acc_train: 0.5241 loss_val: 13.4660 acc_val: 0.1050 time: 16.7837s\n",
            "Epoch: 0014 loss_train: 12.0193 acc_train: 0.1014 loss_val: 4.9674 acc_val: 0.6431 time: 16.6543s\n",
            "Epoch: 0015 loss_train: 2.8647 acc_train: 0.8650 loss_val: 8.2245 acc_val: 0.2405 time: 16.2134s\n",
            "Epoch: 0015 loss_train: 4.6556 acc_train: 0.3505 loss_val: 5.0380 acc_val: 0.7271 time: 16.8900s\n",
            "Epoch: 0016 loss_train: 2.6139 acc_train: 0.7901 loss_val: 5.6659 acc_val: 0.5095 time: 20.9849s\n",
            "Epoch: 0016 loss_train: 4.1937 acc_train: 0.5235 loss_val: 4.8739 acc_val: 0.5916 time: 16.2375s\n",
            "Epoch: 0017 loss_train: 3.9770 acc_train: 0.5684 loss_val: 4.5993 acc_val: 0.7481 time: 16.2360s\n",
            "Epoch: 0017 loss_train: 4.0845 acc_train: 0.8725 loss_val: 7.2932 acc_val: 0.3282 time: 16.2567s\n",
            "Epoch: 0018 loss_train: 5.2905 acc_train: 0.4554 loss_val: 4.3081 acc_val: 0.7118 time: 16.2811s\n",
            "Epoch: 0018 loss_train: 3.4339 acc_train: 0.8859 loss_val: 3.2838 acc_val: 0.7996 time: 16.3386s\n",
            "Epoch: 0019 loss_train: 2.4959 acc_train: 0.8957 loss_val: 2.9355 acc_val: 0.6469 time: 16.1411s\n",
            "Epoch: 0019 loss_train: 1.2932 acc_train: 0.8811 loss_val: 3.7141 acc_val: 0.5248 time: 16.2714s\n",
            "Epoch: 0020 loss_train: 0.4743 acc_train: 0.8487 loss_val: 3.7798 acc_val: 0.6126 time: 17.0483s\n",
            "Epoch: 0020 loss_train: 1.1717 acc_train: 0.8861 loss_val: 3.7957 acc_val: 0.7176 time: 16.2233s\n",
            "Epoch: 0021 loss_train: 1.8072 acc_train: 0.8900 loss_val: 3.5692 acc_val: 0.7786 time: 16.6272s\n",
            "Epoch: 0021 loss_train: 1.8971 acc_train: 0.8903 loss_val: 2.8837 acc_val: 0.8034 time: 16.3966s\n",
            "Epoch: 0022 loss_train: 1.4425 acc_train: 0.8915 loss_val: 1.8662 acc_val: 0.8130 time: 16.2232s\n",
            "Epoch: 0022 loss_train: 0.5023 acc_train: 0.8918 loss_val: 0.9124 acc_val: 0.8454 time: 16.6198s\n",
            "Epoch: 0023 loss_train: 0.0199 acc_train: 0.9938 loss_val: 1.3697 acc_val: 0.8397 time: 16.8003s\n",
            "Epoch: 0023 loss_train: 0.6596 acc_train: 0.8957 loss_val: 1.6917 acc_val: 0.8149 time: 16.0290s\n",
            "Epoch: 0024 loss_train: 0.9461 acc_train: 0.8737 loss_val: 1.4347 acc_val: 0.8092 time: 16.0967s\n",
            "Epoch: 0024 loss_train: 0.5019 acc_train: 0.8966 loss_val: 0.9389 acc_val: 0.7748 time: 16.4606s\n",
            "Epoch: 0025 loss_train: 0.0492 acc_train: 0.9771 loss_val: 1.1660 acc_val: 0.7939 time: 16.0467s\n",
            "Epoch: 0025 loss_train: 0.0031 acc_train: 1.0000 loss_val: 1.6892 acc_val: 0.7519 time: 18.7938s\n",
            "Epoch: 0026 loss_train: 0.0092 acc_train: 0.9976 loss_val: 2.2350 acc_val: 0.7271 time: 16.0353s\n",
            "Epoch: 0026 loss_train: 0.0465 acc_train: 0.9893 loss_val: 2.5520 acc_val: 0.7176 time: 17.2907s\n",
            "Epoch: 0027 loss_train: 0.1764 acc_train: 0.8954 loss_val: 2.4686 acc_val: 0.7195 time: 15.9870s\n",
            "Epoch: 0027 loss_train: 0.0514 acc_train: 0.9917 loss_val: 2.1876 acc_val: 0.7309 time: 17.0736s\n",
            "Epoch: 0028 loss_train: 0.0079 acc_train: 0.9988 loss_val: 1.9640 acc_val: 0.7385 time: 16.3543s\n",
            "Epoch: 0028 loss_train: 0.0022 acc_train: 1.0000 loss_val: 1.7295 acc_val: 0.7538 time: 16.5549s\n",
            "Epoch: 0029 loss_train: 0.0010 acc_train: 1.0000 loss_val: 1.5683 acc_val: 0.7634 time: 15.8953s\n",
            "Epoch: 0029 loss_train: 0.0007 acc_train: 1.0000 loss_val: 1.3980 acc_val: 0.7748 time: 16.4976s\n",
            "Epoch: 0030 loss_train: 0.0005 acc_train: 1.0000 loss_val: 1.2863 acc_val: 0.7901 time: 16.8489s\n",
            "Epoch: 0030 loss_train: 0.0004 acc_train: 1.0000 loss_val: 1.1694 acc_val: 0.8053 time: 16.2560s\n",
            "Epoch: 0031 loss_train: 0.0003 acc_train: 1.0000 loss_val: 1.0965 acc_val: 0.8092 time: 16.0031s\n",
            "Epoch: 0031 loss_train: 0.0002 acc_train: 1.0000 loss_val: 1.0170 acc_val: 0.8187 time: 16.5087s\n",
            "Epoch: 0032 loss_train: 0.0002 acc_train: 1.0000 loss_val: 0.9697 acc_val: 0.8225 time: 16.3995s\n",
            "Epoch: 0032 loss_train: 0.0003 acc_train: 1.0000 loss_val: 0.9168 acc_val: 0.8225 time: 16.4238s\n",
            "Epoch: 0033 loss_train: 0.0004 acc_train: 1.0000 loss_val: 0.8909 acc_val: 0.8225 time: 15.9962s\n",
            "Epoch: 0033 loss_train: 0.0006 acc_train: 1.0000 loss_val: 0.8594 acc_val: 0.8225 time: 16.7778s\n",
            "Epoch: 0034 loss_train: 0.0008 acc_train: 1.0000 loss_val: 0.8494 acc_val: 0.8225 time: 16.0693s\n",
            "Epoch: 0034 loss_train: 0.0012 acc_train: 1.0000 loss_val: 0.8360 acc_val: 0.8244 time: 16.7691s\n",
            "Epoch: 0035 loss_train: 0.0015 acc_train: 1.0000 loss_val: 0.8399 acc_val: 0.8244 time: 15.9759s\n",
            "Epoch: 0035 loss_train: 0.0018 acc_train: 1.0000 loss_val: 0.8407 acc_val: 0.8263 time: 16.9773s\n",
            "Epoch: 0036 loss_train: 0.0019 acc_train: 1.0000 loss_val: 0.8553 acc_val: 0.8263 time: 15.9175s\n",
            "Epoch: 0036 loss_train: 0.0019 acc_train: 1.0000 loss_val: 0.8662 acc_val: 0.8244 time: 16.4939s\n",
            "Epoch: 0037 loss_train: 0.0016 acc_train: 1.0000 loss_val: 0.8871 acc_val: 0.8244 time: 18.1267s\n",
            "Epoch: 0037 loss_train: 0.0014 acc_train: 1.0000 loss_val: 0.9052 acc_val: 0.8244 time: 16.4633s\n",
            "Epoch: 0038 loss_train: 0.0011 acc_train: 1.0000 loss_val: 0.9302 acc_val: 0.8244 time: 16.0106s\n",
            "Epoch: 0038 loss_train: 0.0009 acc_train: 1.0000 loss_val: 0.9528 acc_val: 0.8225 time: 16.5082s\n",
            "Epoch: 0039 loss_train: 0.0007 acc_train: 1.0000 loss_val: 0.9800 acc_val: 0.8206 time: 16.3811s\n",
            "Epoch: 0039 loss_train: 0.0005 acc_train: 1.0000 loss_val: 1.0041 acc_val: 0.8206 time: 16.5065s\n",
            "Epoch: 0040 loss_train: 0.0004 acc_train: 1.0000 loss_val: 1.0314 acc_val: 0.8168 time: 16.1649s\n",
            "Epoch: 0040 loss_train: 0.0004 acc_train: 1.0000 loss_val: 1.0546 acc_val: 0.8149 time: 16.0923s\n",
            "Epoch: 0041 loss_train: 0.0003 acc_train: 1.0000 loss_val: 1.0807 acc_val: 0.8149 time: 16.3141s\n",
            "Epoch: 0041 loss_train: 0.0003 acc_train: 1.0000 loss_val: 1.1015 acc_val: 0.8130 time: 15.9030s\n",
            "Epoch: 0042 loss_train: 0.0003 acc_train: 1.0000 loss_val: 1.1255 acc_val: 0.8130 time: 17.2888s\n",
            "Epoch: 0042 loss_train: 0.0002 acc_train: 1.0000 loss_val: 1.1431 acc_val: 0.8130 time: 16.0438s\n",
            "Epoch: 0043 loss_train: 0.0002 acc_train: 1.0000 loss_val: 1.1649 acc_val: 0.8034 time: 16.1491s\n",
            "Epoch: 0043 loss_train: 0.0002 acc_train: 1.0000 loss_val: 1.1792 acc_val: 0.8015 time: 15.9891s\n",
            "Epoch: 0044 loss_train: 0.0002 acc_train: 1.0000 loss_val: 1.1981 acc_val: 0.7958 time: 17.3480s\n",
            "Epoch: 0044 loss_train: 0.0002 acc_train: 1.0000 loss_val: 1.2090 acc_val: 0.7939 time: 16.0443s\n",
            "Epoch: 0045 loss_train: 0.0002 acc_train: 1.0000 loss_val: 1.2249 acc_val: 0.7882 time: 16.1365s\n",
            "Epoch: 0045 loss_train: 0.0002 acc_train: 1.0000 loss_val: 1.2327 acc_val: 0.7882 time: 16.0694s\n",
            "Epoch: 0046 loss_train: 0.0002 acc_train: 1.0000 loss_val: 1.2450 acc_val: 0.7901 time: 17.1243s\n",
            "Epoch: 0046 loss_train: 0.0002 acc_train: 1.0000 loss_val: 1.2500 acc_val: 0.7882 time: 16.1940s\n",
            "Epoch: 0047 loss_train: 0.0002 acc_train: 1.0000 loss_val: 1.2576 acc_val: 0.7882 time: 16.6232s\n",
            "Epoch: 0047 loss_train: 0.0002 acc_train: 1.0000 loss_val: 1.2602 acc_val: 0.7863 time: 16.4099s\n",
            "Epoch: 0048 loss_train: 0.0002 acc_train: 1.0000 loss_val: 1.2645 acc_val: 0.7863 time: 15.9898s\n",
            "Epoch: 0048 loss_train: 0.0001 acc_train: 1.0000 loss_val: 1.2651 acc_val: 0.7863 time: 16.7149s\n",
            "Epoch: 0049 loss_train: 0.0001 acc_train: 1.0000 loss_val: 1.2667 acc_val: 0.7844 time: 16.3039s\n",
            "Epoch: 0049 loss_train: 0.0001 acc_train: 1.0000 loss_val: 1.2658 acc_val: 0.7844 time: 16.9404s\n",
            "Epoch: 0050 loss_train: 0.0001 acc_train: 1.0000 loss_val: 1.2654 acc_val: 0.7844 time: 16.6064s\n",
            "Epoch: 0050 loss_train: 0.0001 acc_train: 1.0000 loss_val: 1.2634 acc_val: 0.7844 time: 16.2974s\n",
            "Epoch: 0051 loss_train: 0.0001 acc_train: 1.0000 loss_val: 1.2615 acc_val: 0.7844 time: 16.9769s\n",
            "Epoch: 0051 loss_train: 0.0001 acc_train: 1.0000 loss_val: 1.2585 acc_val: 0.7844 time: 17.2400s\n",
            "Epoch: 0052 loss_train: 0.0001 acc_train: 1.0000 loss_val: 1.2560 acc_val: 0.7844 time: 16.0198s\n",
            "Epoch: 0052 loss_train: 0.0001 acc_train: 1.0000 loss_val: 1.2525 acc_val: 0.7863 time: 16.3787s\n",
            "Epoch: 0053 loss_train: 0.0001 acc_train: 1.0000 loss_val: 1.2493 acc_val: 0.7863 time: 16.7356s\n",
            "Epoch: 0053 loss_train: 0.0001 acc_train: 1.0000 loss_val: 1.2454 acc_val: 0.7863 time: 15.8927s\n",
            "Epoch: 0054 loss_train: 0.0001 acc_train: 1.0000 loss_val: 1.2415 acc_val: 0.7863 time: 16.6116s\n",
            "Epoch: 0054 loss_train: 0.0001 acc_train: 1.0000 loss_val: 1.2375 acc_val: 0.7901 time: 16.5030s\n",
            "Epoch: 0055 loss_train: 0.0001 acc_train: 1.0000 loss_val: 1.2334 acc_val: 0.7901 time: 16.3267s\n",
            "Epoch: 0055 loss_train: 0.0001 acc_train: 1.0000 loss_val: 1.2294 acc_val: 0.7901 time: 15.9715s\n",
            "Epoch: 0056 loss_train: 0.0001 acc_train: 1.0000 loss_val: 1.2253 acc_val: 0.7901 time: 16.0869s\n",
            "Epoch: 0056 loss_train: 0.0001 acc_train: 1.0000 loss_val: 1.2213 acc_val: 0.7901 time: 15.9544s\n",
            "Epoch: 0057 loss_train: 0.0001 acc_train: 1.0000 loss_val: 1.2173 acc_val: 0.7901 time: 15.9638s\n",
            "Epoch: 0057 loss_train: 0.0001 acc_train: 1.0000 loss_val: 1.2134 acc_val: 0.7901 time: 16.0592s\n",
            "Epoch: 0058 loss_train: 0.0001 acc_train: 1.0000 loss_val: 1.2095 acc_val: 0.7901 time: 17.4761s\n",
            "Epoch: 0058 loss_train: 0.0001 acc_train: 1.0000 loss_val: 1.2058 acc_val: 0.7901 time: 16.1920s\n",
            "Epoch: 0059 loss_train: 0.0001 acc_train: 1.0000 loss_val: 1.2021 acc_val: 0.7901 time: 16.6094s\n",
            "Epoch: 0059 loss_train: 0.0001 acc_train: 1.0000 loss_val: 1.1985 acc_val: 0.7901 time: 16.5970s\n",
            "Epoch: 0060 loss_train: 0.0001 acc_train: 1.0000 loss_val: 1.1950 acc_val: 0.7920 time: 15.9569s\n",
            "Epoch: 0060 loss_train: 0.0001 acc_train: 1.0000 loss_val: 1.1917 acc_val: 0.7920 time: 16.4518s\n",
            "Epoch: 0061 loss_train: 0.0001 acc_train: 1.0000 loss_val: 1.1884 acc_val: 0.7920 time: 15.9438s\n",
            "Epoch: 0061 loss_train: 0.0001 acc_train: 1.0000 loss_val: 1.1852 acc_val: 0.7920 time: 16.2117s\n",
            "Epoch: 0062 loss_train: 0.0001 acc_train: 1.0000 loss_val: 1.1822 acc_val: 0.7920 time: 16.0191s\n",
            "Epoch: 0062 loss_train: 0.0001 acc_train: 1.0000 loss_val: 1.1792 acc_val: 0.7920 time: 16.7388s\n",
            "Epoch: 0063 loss_train: 0.0001 acc_train: 1.0000 loss_val: 1.1764 acc_val: 0.7920 time: 16.1952s\n",
            "Epoch: 0063 loss_train: 0.0001 acc_train: 1.0000 loss_val: 1.1736 acc_val: 0.7920 time: 16.5246s\n",
            "Epoch: 0064 loss_train: 0.0001 acc_train: 1.0000 loss_val: 1.1710 acc_val: 0.7920 time: 16.4270s\n",
            "Epoch: 0064 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1684 acc_val: 0.7920 time: 17.1974s\n",
            "Epoch: 0065 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1660 acc_val: 0.7920 time: 17.6151s\n",
            "Epoch: 0065 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1636 acc_val: 0.7920 time: 16.0872s\n",
            "Epoch: 0066 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1614 acc_val: 0.7920 time: 15.9179s\n",
            "Epoch: 0066 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1592 acc_val: 0.7920 time: 16.0878s\n",
            "Epoch: 0067 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1571 acc_val: 0.7920 time: 16.0559s\n",
            "Epoch: 0067 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1551 acc_val: 0.7920 time: 15.9652s\n",
            "Epoch: 0068 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1532 acc_val: 0.7920 time: 15.9217s\n",
            "Epoch: 0068 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1514 acc_val: 0.7920 time: 16.1942s\n",
            "Epoch: 0069 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1496 acc_val: 0.7920 time: 16.0004s\n",
            "Epoch: 0069 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1479 acc_val: 0.7920 time: 17.1042s\n",
            "Epoch: 0070 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1463 acc_val: 0.7920 time: 15.9617s\n",
            "Epoch: 0070 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1447 acc_val: 0.7920 time: 16.7895s\n",
            "Epoch: 0071 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1433 acc_val: 0.7920 time: 15.9753s\n",
            "Epoch: 0071 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1418 acc_val: 0.7920 time: 17.0102s\n",
            "Epoch: 0072 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1405 acc_val: 0.7920 time: 16.8912s\n",
            "Epoch: 0072 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1391 acc_val: 0.7920 time: 15.9235s\n",
            "Epoch: 0073 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1379 acc_val: 0.7920 time: 16.4368s\n",
            "Epoch: 0073 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1367 acc_val: 0.7920 time: 16.6819s\n",
            "Epoch: 0074 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1355 acc_val: 0.7939 time: 15.9573s\n",
            "Epoch: 0074 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1344 acc_val: 0.7939 time: 16.4880s\n",
            "Epoch: 0075 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1333 acc_val: 0.7939 time: 16.0386s\n",
            "Epoch: 0075 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1323 acc_val: 0.7939 time: 16.5065s\n",
            "Epoch: 0076 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1313 acc_val: 0.7939 time: 16.1914s\n",
            "Epoch: 0076 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1303 acc_val: 0.7939 time: 16.6868s\n",
            "Epoch: 0077 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1294 acc_val: 0.7939 time: 16.0117s\n",
            "Epoch: 0077 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1286 acc_val: 0.7939 time: 16.5249s\n",
            "Epoch: 0078 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1277 acc_val: 0.7939 time: 15.9070s\n",
            "Epoch: 0078 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1269 acc_val: 0.7939 time: 16.0574s\n",
            "Epoch: 0079 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1261 acc_val: 0.7939 time: 17.0591s\n",
            "Epoch: 0079 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1254 acc_val: 0.7939 time: 16.6893s\n",
            "Epoch: 0080 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1247 acc_val: 0.7939 time: 16.0383s\n",
            "Epoch: 0080 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1240 acc_val: 0.7939 time: 16.5886s\n",
            "Epoch: 0081 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1233 acc_val: 0.7939 time: 15.9757s\n",
            "Epoch: 0081 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1227 acc_val: 0.7939 time: 16.5309s\n",
            "Epoch: 0082 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1220 acc_val: 0.7939 time: 15.9370s\n",
            "Epoch: 0082 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1215 acc_val: 0.7939 time: 16.4855s\n",
            "Epoch: 0083 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1209 acc_val: 0.7939 time: 16.4858s\n",
            "Epoch: 0083 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1203 acc_val: 0.7939 time: 15.9716s\n",
            "Epoch: 0084 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1198 acc_val: 0.7939 time: 16.5291s\n",
            "Epoch: 0084 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1193 acc_val: 0.7920 time: 16.6869s\n",
            "Epoch: 0085 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1188 acc_val: 0.7920 time: 16.5412s\n",
            "Epoch: 0085 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1183 acc_val: 0.7920 time: 15.8998s\n",
            "Epoch: 0086 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1178 acc_val: 0.7920 time: 17.1951s\n",
            "Epoch: 0086 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1174 acc_val: 0.7901 time: 16.6129s\n",
            "Epoch: 0087 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1170 acc_val: 0.7882 time: 16.9081s\n",
            "Epoch: 0087 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1165 acc_val: 0.7882 time: 17.2100s\n",
            "Epoch: 0088 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1161 acc_val: 0.7882 time: 16.4716s\n",
            "Epoch: 0088 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1157 acc_val: 0.7882 time: 17.3056s\n",
            "Epoch: 0089 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1154 acc_val: 0.7882 time: 16.5170s\n",
            "Epoch: 0089 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1150 acc_val: 0.7882 time: 17.2349s\n",
            "Epoch: 0090 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1146 acc_val: 0.7882 time: 16.5642s\n",
            "Epoch: 0090 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1143 acc_val: 0.7882 time: 16.7206s\n",
            "Epoch: 0091 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1140 acc_val: 0.7882 time: 16.3533s\n",
            "Epoch: 0091 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1137 acc_val: 0.7882 time: 17.4155s\n",
            "Epoch: 0092 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1133 acc_val: 0.7882 time: 16.9007s\n",
            "Epoch: 0092 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1130 acc_val: 0.7882 time: 16.9009s\n",
            "Epoch: 0093 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1127 acc_val: 0.7882 time: 16.5724s\n",
            "Epoch: 0093 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1125 acc_val: 0.7882 time: 16.5201s\n",
            "Epoch: 0094 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1122 acc_val: 0.7882 time: 16.3578s\n",
            "Epoch: 0094 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1119 acc_val: 0.7882 time: 16.2264s\n",
            "Epoch: 0095 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1117 acc_val: 0.7882 time: 16.4356s\n",
            "Epoch: 0095 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1114 acc_val: 0.7882 time: 16.2595s\n",
            "Epoch: 0096 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1112 acc_val: 0.7882 time: 16.0490s\n",
            "Epoch: 0096 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1109 acc_val: 0.7882 time: 16.4670s\n",
            "Epoch: 0097 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1107 acc_val: 0.7882 time: 16.6095s\n",
            "Epoch: 0097 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1105 acc_val: 0.7882 time: 16.5526s\n",
            "Epoch: 0098 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1103 acc_val: 0.7882 time: 16.2710s\n",
            "Epoch: 0098 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1100 acc_val: 0.7901 time: 16.3807s\n",
            "Epoch: 0099 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1098 acc_val: 0.7901 time: 15.9681s\n",
            "Epoch: 0099 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1096 acc_val: 0.7901 time: 16.1143s\n",
            "Epoch: 0100 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1094 acc_val: 0.7901 time: 16.1618s\n",
            "Epoch: 0100 loss_train: 0.0000 acc_train: 1.0000 loss_val: 1.1093 acc_val: 0.7901 time: 16.4581s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 8810.2049s\n",
            "<__main__.Args object at 0x795e1f95bf40>\n",
            "\t=== testing ===\n",
            "with pro (0.816793893129771, 0.816793893129771)\n"
          ]
        }
      ],
      "source": [
        "class Args:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.alpha = kwargs.get('alpha', 0.064)\n",
        "        self.beta = kwargs.get('beta', 5)\n",
        "        self.lambda_ = kwargs.get('lambda_', 320)\n",
        "        self.debug = kwargs.get('debug', False)\n",
        "        self.lr = kwargs.get('lr', 0.001)\n",
        "        self.lr_adj = kwargs.get('lr_adj', 0.001)\n",
        "        self.gamma = kwargs.get('gamma', 16)\n",
        "        self.phi = kwargs.get('phi', 0.5)\n",
        "        self.outer_steps = kwargs.get('outer_steps', 1)\n",
        "        self.inner_steps = kwargs.get('inner_steps', 100)\n",
        "        self.weight_decay = kwargs.get('weight_decay', 0.0001)\n",
        "        self.epochs = kwargs.get('epochs', 15)\n",
        "        self.symmetric = kwargs.get('symmetric', True)\n",
        "        self.only_gcn = kwargs.get('only_gcn', False)\n",
        "        self.pos_weight = kwargs.get('pos_weight', False)\n",
        "        self.n_classes = kwargs.get('n_classes', None)\n",
        "\n",
        "args_dict = {\n",
        "    'alpha': 0.01,\n",
        "    'beta': 2,\n",
        "    'lambda_': 0.01,\n",
        "    'debug': False,\n",
        "    'lr_adj': 0.01,\n",
        "    'gamma': 1,\n",
        "    'phi': 0,\n",
        "    'outer_steps': 1,\n",
        "    'inner_steps': 2,\n",
        "    'weight_decay': 0.0005,\n",
        "    'epochs': 100,\n",
        "    'symmetric': True,\n",
        "    'only_gcn': False,\n",
        "    'pos_weight': False,\n",
        "    'n_classes': 4\n",
        "}\n",
        "args = Args(**args_dict)\n",
        "\n",
        "# @title\n",
        "import numpy as np\n",
        "from scipy.sparse import coo_matrix\n",
        "\n",
        "# Load the dense adjacency matrix from the .npy file\n",
        "arr = ['1.0']#,'0.2','0.4','0.6','0.8','1.0']#,'1.2','1.4','1.6','1.8']\n",
        "new_args =  ['-he_dim', '50', '-hyla_dim', '1000', '-dataset', 'airport', '-order', '2', '-lambda_scale', '0.01', '-lr_e', '0.1', '-lr_c', '0.1', '-epochs', '100','-quiet']\n",
        "file_path3 = '/content/drive/MyDrive/attacks//random/airport/labels.npy'\n",
        "file_path2 = '/content/drive/MyDrive/attacks//random/airport/features.npy'\n",
        "file_path4 = '/content/drive/MyDrive/attacks//random/airport/idx_train.npy'\n",
        "file_path5 = '/content/drive/MyDrive/attacks//random/airport/idx_test.npy'\n",
        "file_path6 = '/content/drive/MyDrive/attacks//random/airport/idx_val.npy'\n",
        "\n",
        "for i in arr:\n",
        "  file_path = f'/content/drive/MyDrive/original_datasets/airport_adj_train.npy'\n",
        "  dense_adj_matrix = np.load(file_path,allow_pickle=True)\n",
        "  feat_matrix = np.load(file_path2,allow_pickle=True)\n",
        "  idx_train = np.load(file_path4,allow_pickle=True).tolist()\n",
        "  idx_test = np.load(file_path5,allow_pickle=True).tolist()\n",
        "  idx_val = np.load(file_path6,allow_pickle=True).tolist()\n",
        "  labels = torch.tensor(np.load(file_path3))\n",
        "  labels = labels.to(torch.long)\n",
        "  sparse_adj_matrix = coo_matrix(dense_adj_matrix)\n",
        "  coo_indices = torch.tensor(feat_matrix.nonzero())\n",
        "  coo_values = torch.from_numpy(feat_matrix[feat_matrix.nonzero()])\n",
        "  size = tuple(feat_matrix.shape)  # Provide the size as a tuple of integers\n",
        "  #arr_temp = main(new_args, adj_arg=sparse_adj_matrix,feat_arg=torch.sparse_coo_tensor(coo_indices, coo_values, size=size, dtype=torch.double),labels=torch.tensor(labels), idx_train=idx_train, idx_val=idx_val, idx_test=idx_test)\n",
        "  arr_temp2 = main(new_args, adj_arg=sparse_adj_matrix,feat_arg=torch.sparse_coo_tensor(coo_indices, coo_values, size=size, dtype=torch.double),labels=torch.tensor(labels), idx_train=idx_train, idx_val=idx_val, idx_test=idx_test,proGNN=True, prognn_args=args)\n",
        "  # print(\"without pro\", arr_temp, i)\n",
        "  print(\"with pro\", arr_temp2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
